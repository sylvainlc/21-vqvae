\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{authblk,url}
\usepackage{amssymb,amsmath,amsthm,twoopt,xargs,mathtools}
\usepackage{times,dsfont,ifthen}
\usepackage{fancyhdr,xcolor}

\title{}
\date{}

%\author[$\wr$]{Mathis Chagneux}
\author[$\dag$]{XXX}
%\affil[$\wr$]{{\small LTCI, T\'el\'ecom Paris, Institut Polytechnique de Paris, Palaiseau.}}

\affil[$\dag$]{{\small CMAP, \'Ecole Polytechnique, Institut Polytechnique de Paris, Palaiseau.}}

\lhead{}
\rhead{}


\usepackage{geometry}
\pagestyle{fancy}

\def\dimX{d}
\def\dimY{m}
%\def\Xset{\mathsf{X}}
\def\Xset{\mathbb{R}^d}
\def\Yset{\mathsf{Y}}
\newcommand{\mk}{\kernel{G}}
\newcommand{\hk}{\kernel{Q}}
\newcommand{\md}[1]{g_{#1}}
\newcommand{\SmoothFigSize}{0.27}

\newcommand{\logllh}[1]{\ell_{#1}}
\newcommand{\llh}[1]{\mathsf{L}_{#1}}
\newcommand{\testf}{\mathsf{h}}

\newcommandx\filtderiv[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\eta_{#2}}
	{\eta_{#2}^\N}
}
\newcommand{\pred}[1]{\pi_{#1}}
\newcommand{\parvec}{\theta}
\newcommand{\parspace}{\Theta}
\newcommand{\tstatletter}{\kernel{T}}
\newcommand{\retrok}{\kernel{D}}
\newcommandx\tstat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\tau_{#2}^{#1}}
}
\newcommandx\tstathat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\widehat{\tau}_{#2}^{#1}}
}
\newcommand{\af}[1]{h_{#1}} 
\newcommand{\deriv}{\nabla_{\parvec}}

\newcommand{\kernel}[1]{\mathbf{#1}}
\newcommand{\bmf}[1]{\set{F}(#1)}
\newcommand{\set}[1]{\mathsf{#1}}

\newcommandx{\bk}[2][1=]{ 
\ifthenelse{\equal{#1}{}}
{\overleftarrow{\kernel{Q}}_{#2}}
{\overleftarrow{\kernel{Q}}_{#2}^{#1}}
}

\newcommandx{\bkhat}[2][1=]{ 
\ifthenelse{\equal{#1}{}}
{\widehat{\kernel{Q}}_{#2}}
{\widehat{\kernel{Q}}_{#2}^{#1}}
}

\newcommand{\lk}{\kernel{L}}
\newcommand{\idop}{\operatorname{id}}
\newcommand{\hd}[1]{q_{#1}} 
\newcommand{\hdhat}[1]{\widehat{q}_{#1}} 


\newcommand{\addf}[1]{\termletter_{#1}}
\newcommand{\addfc}[1]{\underline{\termletter}_{#1}}
\newcommand{\adds}[1]{\af{#1}}
\newcommand{\term}[1]{\termletter_{#1}}
\newcommand{\termletter}{\tilde{h}}
\newcommand{\N}{N}
\newcommand{\partpred}[1]{\pi_{#1}^\N}
\newcommand{\tstattil}[2]{\tilde{\tau}_{#2}^{#1}}
\newcommandx{\K}[1][1=]{
\ifthenelse{\equal{#1}{}}{{\kletter}}{{\widetilde{\N}^{#1}}}}
\newcommand{\hkup}{\bar{\varepsilon}}
\newcommand{\bi}[3]{J_{#1}^{(#2, #3)}}
\newcommand{\bihat}[3]{\widehat{J}_{#1}^{(#2, #3)}}

\newcommand{\kletter}{\widetilde{\N}}

\def\sigmaX{\mathcal{X}}
\def\sigmaY{\mathcal{Y}}
\def\1{\mathds{1}}
\def\pE{\mathbb{E}}
\def\pP{\mathbb{P}}
\def\plim{\overset{\pP}{\longrightarrow}}
\def\dlim{\Longrightarrow}
\def\gauss{\mathcal{N}}


\newcommand{\esssup}[2][]
{\ifthenelse{\equal{#1}{}}{\left\| #2 \right\|_\infty}{\left\| #2 \right\|^2_{\infty}}}


\newcommand{\swght}[2]{\ensuremath{\omega_{#1}^{#2}}}

\newtheorem{assumptionA}{\textbf{A}\hspace{-3pt}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\newcommand{\iid}{i.i.d.}

\newcommand{\smwght}[3]{\tilde{\omega}_{#1|#2}^{#3}}
\newcommand{\smwghtfunc}[2]{\tilde{\omega}_{#1|#2}}

\newcommand{\smpart}[3]{\ensuremath{\tilde{\xi}_{#1|#2}^{#3}}}
\def\aux{{\scriptstyle{\mathrm{aux}}}}
\newcommand{\bdm}{\mathsf{TwoFilt}_{bdm}}
\newcommand{\fwt}{\mathsf{TwoFilt}_{fwt}}

\newcommand{\kiss}[3][]
{\ifthenelse{\equal{#1}{}}{r_{#2|#3}}
{\ifthenelse{\equal{#1}{fully}}{r^{\star}_{#2|#3}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2|#3}}{\mathrm{erreur}}}}}

\newcommand{\chunk}[4][]%
{\ifthenelse{\equal{#1}{}}{\ensuremath{{#2}_{#3:#4}}}{\ensuremath{#2^#1}_{#3:#4}}
}

\newcommand{\kissforward}[3][]
{\ifthenelse{\equal{#1}{}}{p_{#2}}
{\ifthenelse{\equal{#1}{fully}}{p^{\star}_{#2}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2}}{\mathrm{erreur}}}}}

\newcommand{\instrpostaux}[1]{\ensuremath{\upsilon_{#1}}}
\newcommandx\post[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\phi_{#2}}
	{\phi_{#2}^\N}
}

\newcommandx\posthat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\widehat{\phi}_{#2}}
	{\widehat{\phi}_{#2}^\N}
}

\newcommand{\adjfunc}[4][]
{\ifthenelse{\equal{#1}{}}{\ifthenelse{\equal{#4}{}}{\vartheta_{#2|#3}}{\vartheta_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{smooth}}{\ifthenelse{\equal{#4}{}}{\tilde{\vartheta}_{#2|#3}}{\tilde{\vartheta}_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{fully}}{\ifthenelse{\equal{#4}{}}{\vartheta^\star_{#2|#3}}{\vartheta^\star_{#2|#3}(#4)}}{\mathrm{erreur}}}}}

\newcommand{\XinitIS}[2][]
{\ifthenelse{\equal{#1}{}}{\ensuremath{\rho_{#2}}}{\ensuremath{\check{\rho}_{#2}}}}
\newcommand{\adjfuncforward}[1]{\vartheta_{#1}}
\newcommand{\rmd}{\ensuremath{\mathrm{d}}}
\newcommand{\eqdef}{\ensuremath{:=}}
\newcommand{\eqsp}{\;}
\newcommand{\ewght}[2]{\ensuremath{\omega_{#1}^{#2}}}
\newcommand{\ewghthat}[2]{\ensuremath{\widehat{\omega}_{#1}^{#2}}}
\newcommand{\epart}[2]{\ensuremath{\xi_{#1}^{#2}}}
\newcommand{\filt}[2][]%
{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\phi_{#2}}}{\ensuremath{\phi_{#1,#2}}}%
}
\newcommand{\Xinit}{\ensuremath{\chi}}
\newcommand{\sumwght}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\Omega_{#2}}}{\ensuremath{\Omega_{#2}^{(#1)}}}}
\newcommand{\sumwghthat}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\widehat{\Omega}_{#2}}}{\ensuremath{\widehat{\Omega}_{#2}^{(#1)}}}}

\newcounter{hypH}
\newenvironment{hypH}{\refstepcounter{hypH}\begin{itemize}
\item[{\bf H\arabic{hypH}}]}{\end{itemize}}

\newcommand{\marginalset}{\mathsf{U}}

\newcommand{\calF}[2]{\mathcal{F}_{#1}^{#2}}
\newcommand{\calG}[2]{\mathcal{G}_{#1}^{#2}}
\newcommand{\Uset}{\mathsf{U}}
\newcommand{\tcalF}[2]{\widetilde{\mathcal{F}}_{#1}^{#2}}
\newcommand{\tcalG}[2]{\widetilde{\mathcal{G}}_{#1}^{#2}}

\newcommand{\kernelmarg}{\mathbf{R}}

\newcommand{\pplim}{\overset{\pP}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\ddlim}{\overset{\mathcal{D}}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\aslim}{\overset{\pP\mathrm{-a.s.}}{ \underset{\N \to \infty}{\longrightarrow}}}

\newcommand{\qg}[1]{\ell_{#1}}
\newcommand{\hatqg}[1]{\mathsf{\ell}_{#1}}

\newcommand{\sfd}{\mathsf{d}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\e}{\text{e}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\frob}{:}
\newcommand{\rme}{\mathrm{e}}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}


\subsection*{Experiments} 
List of experiments \& datasets to validate the new approach and compare to vq-vae \url{https://docs.google.com/document/d/1a7lhplsBraTD3qmvsHXdoPmQj00YijT2XMWpHEFeb58/edit}

\subsection*{Draft for the new approach}
 Let $(Y^1,\ldots,Y^n)$ be the observations in $\rset^q$ (a sequence of structured data, not necessarily some time series).
\paragraph{Encoder.} Consider an encoding function $f_\theta$ (proper neural network-based encoder  to be defined when the model is fixed) and write
$$
(z_\rme^1,\ldots, z_\rme^m) = f_\theta(Y^{1:n})\eqsp.
$$
Note that the function $f_\theta$ does not build as many encoding states $(z_\rme^1,\ldots, z_\rme^m)$ as the number of observations. 
\paragraph{Discrete latent states.} The crucial step to estimate the parameter is to compute 
$$
\nabla_\theta\mathcal{L}(\theta) = \nabla_\theta \mathbb{E}_{\hat{\theta}_p}\left[\log p_{\theta}(z^{1:m}_q,Y^{1:n})  \middle | Y^{1:n}\right]\eqsp,
$$
i.e. to compute an expectation under the distribution of the sequence $z^{1:m}_q$ given the observations $Y^{1:n}$. Following the original vq-vae, we then propose a model for the conditional distribution of the sequence $z^{1:m}_q$ given $Y^{1:n}$ (and not for the prior distribution of the discrete states). Consider $(\rme^1,\ldots,\rme^K)$ the K embedding vectors. Let $Y^{-k}$ be observations in a neighborhood of $Y^k$. From $(z_\rme^1,\ldots, z_\rme^m)$ and the observations, the model builds a Markov chain (we may consider more comple dependencies) $(z_q^1,\ldots, z_q^n)$ taking values in $\{\rme^1,\ldots,\rme^K\}$ such that:
$$
\mathbb{P}_\theta(z_q^1 = \rme^j|Y^{1:n}) = \mu_\theta[z_\rme^1,Y^{-1}](\rme^j) \eqsp,\quad \mathbb{P}_\theta(z_q^{k+1} = \rme^j | z_q^{k} =  \rme^\ell,Y^{1:n}) = Q_{\theta}[z_\rme^{k},z_\rme^{k+1},Y^{-k}](\rme^\ell,\rme^j)\eqsp, 
$$
where, for all $z,z'$, $\{\mu_{\theta}[z,Y^{-1}](\rme^j)\}_{1\leqslant j\leqslant K}$ is a probability vector and $\{Q_{\theta}[z,z',Y^{-k}](\rme^\ell,\rme^j)\}_{1\leqslant j,\ell\leqslant K}$ is a transition matrix on $\{\rme^1,\ldots, \rme^K\}$.  
\paragraph{Decoder.} The conditional law of the observation $Y_{k}$ given $\mathcal{F}_k$ (proper $\sigma$-field to be defined when the model is fixed, a neihnorhood of observations and latent states) is written
$$
Y^{k} = \mu^{z_q^{k}}_\theta(Y^{-k}) + \sigma^{z_q^{k}}_\theta(Y^{-k})\varepsilon_{k}\eqsp,
$$
where the $\{\varepsilon_{k}\}_{1\leqslant k\leqslant n}$ are i.i.d. standard Gaussian random variables in $\rset^q$ and $(\mu^{j},\sigma^{j}_\theta)$ are outputs of some neural network architecture to be defined  (proper neural network-based encoder  to be defined when the model is fixed).
\paragraph{Training.}
The gradient step is given by 
\begin{multline*}
\nabla_\theta\mathcal{L}(\theta) = \nabla_\theta \mathbb{E}_{\hat{\theta}_p}\left[\log p_{\theta}(z^{1:m}_q,Y^{1:n})  \middle | Y^{1:n}\right] \\
=  \nabla_\theta \mathbb{E}_{\hat{\theta}_p}\left[\log p_{\theta}(z^{1:m}_q)  \middle | Y^{1:n}\right]  + \nabla_\theta \mathbb{E}_{\hat{\theta}_p}\left[\log p_{\theta}(Y^{1:n}|z^{1:m}_q)  \middle | Y^{1:n}\right]\eqsp.
\end{multline*}
The second term is easy to compute (depending on the complexity of $ p_{\theta}(Y^{1:n}|z^{1:m}_q)$ but we may start with something easy) as the computational expecation is computed with the conditional kernel. The second term is trickier, it is the term trained in a separate learning framework in vq-vae. We should use the explicit marginalization 
$$
 p_{\theta}(z^{1:m}_q) \sim \int p_{\theta}(z^{1:m}_q|y^{1:n})p_*(\rmd y^{1:n})
$$
and provide a Monte Carlo estimate based on samples from $p_\star$, i.e. samples from the dataset.



\clearpage
\newpage
\textcolor{red}{The notes below are not useful anymore... for now!}


\section{Introduction}
\label{sec:intro}

\section{Mixture-based vq-vae}
\label{sec:model:mixture}
\subsection{Model and architectures}
Let $X_I \in \rset^d$ be the input data and $Y$ be the observations in $\rset^q$.  Several frameworks.

\paragraph{Encoder.} Consider an encoding function $f_\theta: \rset^d \to \rset^{m}$ (proper neural network-based encoder  to be defined when the model is fixed) and write
$$
z_\rme = f_\theta(X_I)\eqsp.
$$


\paragraph{Discrete latent states.} Consider $(\rme^1,\ldots,\rme^K)$ the K embedding vectors in $\rset^m$. From $z_\rme$ the model builds a latent state $z_q$ taking values in $\{\rme^1,\ldots,\rme^K\}$ such that:%(this set is identified with $\{1,\ldots, K\}$ in throughout the paper)  
$$
\mathbb{P}_\theta(z_q= \rme^j) = \rho^{}_{\theta}(z_\rme;\rme^j) \eqsp,
$$
where, for all $z\in\rset^m$, $\{\rho_{\theta}(z;\rme^j)\}_{1\leqslant j\leqslant K}$ is a probability vector on  $\{\rme^1,\ldots,\rme^K\}$.
\paragraph{Decoder.} The conditional law of the observation $Y$ given $\mathcal{F}$ (proper $\sigma$-field to be defined when the model is fixed) is written
$$
Y = \mu_\theta(z_q) + \sigma_\theta(z_q)\varepsilon\eqsp,
$$
where the $\varepsilon$ is a standard Gaussian random variables in $\rset^q$ and for all $z\in\rset^m$, $\{\mu_\theta(z),\sigma_\theta(z)\}$ are outputs of some neural network architecture to be defined  (proper neural network-based encoder  to be defined when the model is fixed).

\subsection{Loss function}
The (complete) loglikelihood of the model (conditionally on $X_I$ which is dropped from the notations) is given by
$$
\log p_{\theta}(z_q,Y) =  \log \rho^{}_{\theta}(z_\rme;z_q) +  \log \varphi_{\mu^{}_\theta (z_q),\sigma^{}_\theta (z_q)}(Y) \eqsp,
$$
where $\varphi_{\mu,\sigma}$ is the Gaussian probability density function with mean $\mu$ and variance $\sigma^2$. The posterior distribution of $z_q$ given $Y$ is 
$$
\mathbb{P}_\theta(z_q = \rme^j|Y) = \frac{\rho^{}_{\theta}(z_\rme;\rme^j)\varphi_{\mu_\theta(\rme^j),\sigma^{}_\theta(\rme^j)}(Y)}{\sum_{k=1}^{K}\rho^{}_{\theta}(z_\rme;\rme^k)\varphi_{\mu_\theta(\rme^k),\sigma_\theta(\rme^k)}(Y)} = \omega^j_{z}\eqsp.
$$
Then, consider the loss function (Fisher or EM based update),
$$
\mathcal{L}(\theta) = \mathbb{E}\left[\log p_{\theta}(z_q,Y)  \middle | Y\right] = \sum_{j=1}^K  \omega^j_{z}\log p_{\theta}(\rme^j,Y)=  \sum_{j=1}^K  \omega^j_{z}\left\{\log \rho^{}_{\theta}(z_\rme;\rme^j) +  \log \varphi_{\mu^{}_\theta(\rme^j),\sigma^{}_\theta(\rme^j)}(Y)\right\}\eqsp.
$$
An alternative is to compute the estimate $\mathbb{E}_\theta[z_q|Y] = \sum_{j=1}^K  \omega^j_{z}\rme^j$.

\section{Application to time series}
\label{sec:model}
Following the previous section, the crucial step to estimate the parameter is to compute 
$$
\mathcal{L}(\theta) = \mathbb{E}\left[\log p_{\theta}(z^{1:n}_q,Y^{1:n})  \middle | Y^{1:n}\right]\eqsp,
$$
i.e. to compute an expectation under the distribution of the sequence $z^{1:n}_q$ given the observations $Y^{1:n}$.

In the case where $z^{1:n}_q$ is a Markov chain, we know that conditionally on $Y^{1:n}$, $z^{1:n}_q$ is a Markov chain which revolves backwards in time. Instead of computing the backward Markov kernel from the prior kernel of $z^{1:n}_q$ (see the computations below) we propose to directly model the backward Markov kernel. 

For all $z\in\rset^m$, $y^{1:n}$, let $\{\mu^{z,y^{\underline\kappa(n):n}}_{\theta}(j)\}_{1\leqslant j\leqslant K}$ be a probability vector and $Q_k^{z,y^{\underline\kappa(k):\overline\kappa(k)}}$ be a transition matrix on $\{\rme^1,\ldots, \rme^K\}$. For each $k$,$\{\underline\kappa(k)\ldots,\overline\kappa(k)\}$ is the time frame around $k$ selecting the observations used as an input for  $Q_k^{z,y^{\underline\kappa(k):\overline\kappa(k)}}$.

\subsection{Model and architectures}
Let $X_I \in \rset^d$ be the input data and $(Y^1,\ldots,Y^n)$ be the observations in $\rset^q$.  Several frameworks.
\begin{enumerate}
\item $X_I$ contains weather forecasts and building management settings for the next 3 days. The observations $(Y^1,\ldots,Y^n)$  are the temperatures/energy consumptions to predict.
\item Visual language modeling. 
\end{enumerate}

\paragraph{Encoder.} Consider an encoding function $f_\theta: \rset^d \to \rset^{m\times n}$ (proper neural network-based encoder  to be defined when the model is fixed) and write
$$
(z_\rme^1,\ldots, z_\rme^n) = f_\theta(X_I)\eqsp.
$$
The function $f_\theta$ builds as many encoding states $(z_\rme^1,\ldots, z_\rme^n)$ as the number of observations. 

\paragraph{Discrete latent states.} Consider $(\rme^1,\ldots,\rme^K)$ the K embedding vectors in $\rset^m$. From $(z_\rme^1,\ldots, z_\rme^n)$ the model builds a Markov chain $(z_q^1,\ldots, z_q^n)$ taking values in $\{\rme^1,\ldots,\rme^K\}$ (this set is identified with $\{1,\ldots, K\}$ in the following)  such that:
$$
\mathbb{P}_\theta(z_q^1 = j) = \mu^{z_\rme^1}_{\theta}(j) \eqsp,\quad \mathbb{P}_\theta(z_q^{k+1} = j | z_q^{k} =  \ell) = Q^{z_\rme^{k+1}}_{\theta}(\ell,j)\eqsp, 
$$
where, for all $z\in\rset^m$, $\{\mu^z_{\theta}(j)\}_{1\leqslant j\leqslant K}$ is a probability vector and $\{Q^{z}_{\theta}(\ell,j)\}_{1\leqslant j,\ell\leqslant K}$ is a transition matrix on $\{1,\ldots, K\}$.
\paragraph{Decoder.} The conditional law of the observation $Y_{k+1}$ given $\mathcal{F}_k$ (proper $\sigma$-field to be defined when the model is fixed) is written
$$
Y^{k+1} = \mu^{z_q^{k+1}}_\theta \!\!\!\!\!(Y^{k}) + \sigma^{z_q^{k+1}}_\theta\!\!\!\!\!(Y^{k})\varepsilon_{k+1}\eqsp,
$$
where the $\{\varepsilon_{k}\}_{1\leqslant k\leqslant n}$ are i.i.d. standard Gaussian random variables in $\rset^m$ and $(\mu^{j},\sigma^{j}_\theta)$ are outputs of some neural network architecture to be defined  (proper neural network-based encoder  to be defined when the model is fixed).

\subsection{Loss function}
The (complete) loglikelihood of the model (conditionally on $X_I$ which is dropped from the notations) is given by
\begin{align*}
\log p_{\theta}(z_q^{1:n},Y^{1:n}) =  \log \mu^{z_\rme^1}_{\theta}(z_q^1) + \sum_{k=0}^{n-1} \log  Q^{z_\rme^{k+1}}_{\theta}(z_q^k,z_q^{k+1})
+ \sum_{k=0}^{n-1} \log \varphi_{\mu^{z_q^{k+1}}_\theta\!\!\!\!\!(Y^{k}),\sigma^{z_q^{k+1}}_\theta\!\!\!\!\!(Y^{k})}(Y_{k+1}) \eqsp,
\end{align*}
where $\varphi_{\mu,\sigma}$ is the Gaussian probability density function with mean $\mu$ and variance $\sigma^2$. The posterior distribution of $(z_q,z_q^{k+1})$ given $Y^{1:n}$ is written (see computation below)
$$
\mathbb{P}_\theta(z^k_q = \rme^j,z^k_q = \rme^\ell|Y^{1:n}) =  \omega^{j,\ell}_{z,k,k+1|n}\eqsp.
$$
Then, consider the loss function (Fisher or EM based update),
\begin{align*}
\mathcal{L}(\theta) &= \mathbb{E}\left[\log p_{\theta}(z^{1:n}_q,Y^{1:n})  \middle | Y^{1:n}\right]\eqsp,\\
& =  \sum_{j=1}^K\omega^{j}_{z,1|n}\log \mu^{z_\rme^1}_{\theta}(\rme^j) + \sum_{k=0}^{n-1}\sum_{j,\ell=1}^K   \omega^{j,\ell}_{z,k,k+1|n}\left\{ \log  Q^{z_\rme^{k+1}}_{\theta}(\rme^j,\rme^\ell) +  \log \varphi_{\mu^{\rme^\ell}_\theta (Y^{k}),\sigma^{\rme^\ell}_\theta(Y^{k})}(Y_{k+1})  \right\}\eqsp,
\end{align*}
where $\omega^{j}_{1,z|n} = \sum_{\ell=1}^K\omega^{j,\ell}_{z,1,2|n}$


%
%If we use a score-based estimation procedure, the parameter update is based on 
%$$
%\nabla_{\theta}\log p_{\theta,\rme^{1:n}}(Y^{1:n})\eqsp.
%$$
%An estimation of this gradient is given by 
%$$
%\nabla_{\theta} \log p_{\theta,\rme^{1:n}}(\hat z_q^{1:n},Y^{1:n})\eqsp,
%$$
%where $\hat z_q^{1:n} \sim p_{\theta,\rme^{1:n}}(z_q^{1:n}|X_I,Y^{1:n})$. 

\paragraph{Computation of the posterior weights}
Such a sample can be obtained with Baum-Welch algorithm.
First compute during the forward pass the discrete distributions $\phi_k$, where $\phi_k$ denotes the distribution of $z_q^k$ given $(X_I,Y^{1:k})$ as follows.
\begin{enumerate}
\item For all $1\leqslant j \leqslant K$, 
$$
\phi_1(j) \propto \mu^{z_\rme^1}_{\theta}(j) \varphi_{\mu^{z_q^{1}}_\theta,\sigma^{z_q^{1}}_\theta}(Y_1)\eqsp.
$$
\item For all $1\leqslant j \leqslant K$ and  $1\leqslant k \leqslant n$,
$$
\phi_{k+1}(j) \propto \sum_{\ell=1}^K \phi_{k}(\ell)  Q^{z_\rme^{k+1}}_{\theta}(\ell,j) \varphi_{\mu^{j}_\theta (Y^{k}),\sigma^{j}_\theta(Y^{k})}(Y_{k+1})\eqsp.
$$
\end{enumerate}
Then, the weights $ \omega^{j,\ell}_{z|k,k+1}$ are computed recursively backward
%Then, to sample $\hat z_q^{1:n} \sim p_{\theta,\rme^{1:n}}(z_q^{1:n}|X_I,Y^{1:n})$, the Baum-Welch proceeds recursively backward as follows.
\begin{enumerate}
\item For all $1\leqslant \ell,j \leqslant K$, 
$$
\omega^{j,\ell}_{z,n-1,n|n} \propto \phi_{n-1}(j)Q^{z_\rme^{n}}_{\theta}(j,\ell) \varphi_{\mu^{\ell}_\theta(Y^{n-1}),\sigma^{\ell}_\theta(Y^{n-1})}(Y_{n})\phi_{n}(\ell)   \eqsp.
$$
\item For all  $1\leqslant k \leqslant n-2$,
$$
\omega^{j,\ell}_{z,k,k+1|n} \propto \phi_{k}(j)Q^{z_\rme^{k+1}}_{\theta}(j,\ell) \varphi_{\mu^{\ell}_\theta(Y^{k}),\sigma^{\ell}_\theta(Y^{k})}(Y_{k+1})\omega^{\ell}_{z,k+1|n}  \eqsp,
$$
where $\omega^{\ell}_{z,k+1|n} = \sum_{m=1}^K\omega^{\ell,m}_{z,k+1,k+2|n}$.
\end{enumerate}

\subsection{Training algorithm}
%\bibliographystyle{apalike}
%\bibliography{vqvae}


\end{document}
