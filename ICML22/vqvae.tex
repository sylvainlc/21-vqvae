%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{dsfont}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2022}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2022}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\rset}{\mathbb{R}}
\newcommand{\latentcont}{\mathsf{z}_e}
\newcommand{\latentcontpred}{\mathsf{h}_e}
\newcommand{\latentdis}{\mathsf{z}_q}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\embedspace}{\mathcal{E}}
\newcommand{\embed}{\rme}
\newcommand{\bckw}{\tilde{q}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\tcr}[1]{\textcolor{red}{#1}}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Diffusion bridges for VQVAE}

\begin{document}

\twocolumn[
\icmltitle{Diffusion bridges  vector quantized variational autoencoders}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Max Cohen}{*}
\icmlauthor{Guillaume Quispe}{$\dag$}
\icmlauthor{Sylvain Le Corff}{*}
\icmlauthor{Charles Ollion}{$\dag$}
\icmlauthor{\'Eric Moulines}{$\dag$}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{*}{Samovar, T\'el\'ecom SudParis, D\'epartement CITI, Palaiseau, France}
\icmlaffiliation{$\dag$}{Centre de Math\'ematiques Appliqu\'ees, \'Ecole polytechnique,Palaiseau, France}


\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
%
Vector Quantised-Variational AutoEncoders (VQ-VAE) are generative models based on discrete latent representations of the data, where inputs are mapped to a finite set of learned embeddings.
To generate new samples, an autoregressive prior distribution over the discrete states must be trained separately. This prior is generally very complex and leads to very slow generation. In this work, we propose a new model to train the prior and the encoder/decoder networks simultaneously. We build a diffusion bridge between a continuous coded vector and a non-informative prior distribution.  The latent discrete states are then given as random functions of these continuous vectors. We show that our model is competitive with the autoregressive prior on the mini-Imagenet dataset and is very efficient in both optimization and sampling. Our framework also extends the standard VQ-VAE and enables end-to-end training.
 \end{abstract}
% 
\section{Introduction}
Variational AutoEncoders (VAE) have emerged as important generative models based on latent representations of the data. 
While the latent states are usually continuous vectors, Vector Quantised Variational AutoEncoders (VQ-VAE) have demonstrated the usefulness of discrete latent spaces and have been successfully applied in image and speech generation \cite{oord2017neural, esser2021taming, ramesh2021zero}. %In a VQ-VAE, the distribution of inputs $x\in\rset^m$ is assumed to depend on a hidden discrete state $\latentdis$ that takes $K$ possible values in $\embedspace = \{\embed_1,\ldots,\embed_K\}$ with $\embed_k\in\rset^d$ for all $1\leqslant k \leqslant K$. Note that for images there are multiple discrete latent states $\latentdis$, typically organized as 2D lattices.

In a VQ-VAE, the distribution of the inputs is assumed to depend on a hidden discrete state. Large scale image generation VQ-VAEs use for instance multiple discrete latent states, typically organized as 2-dimensional lattices. In the original VQ-VAE, the authors propose a variational approach to approximate the posterior distribution of the discrete states given the observations. The variational distribution takes as input the observation, which is passed through an encoder. The discrete latent variable is then computed by a nearest neighbour procedure that maps the encoded vector to the nearest discrete embedding. 

% The variational distribution is thus given by:
%  $$
%  q_\varphi(\latentdis|x) = \delta_{\embed_{k^*_x}}(\latentdis)\,,
%  $$
%  where $\delta$ is the Dirac mass and
%  $$
%  k^*_x = \mathrm{argmin}_{1\leqslant k \leqslant K}\left\{\|\latentcont(x)-\embed_k\|_2\right\}\,.
%  $$
 It has been argued that the success of VQ-VAEs lies in the fact that they do not suffer from the usual posterior collapse of VAEs  \cite{oord2017neural}. However, the implementation of VQ-VAE involves many practical tricks and still suffers from several limitations. First, the quantisation step leads the authors to propose a rough approximation of the gradient of the loss function by copying gradients from the decoder input to the encoder output. Second, the prior distribution of the discrete variables is initially assumed to be uniform when training the VQ-VAE. In a second training step, very high-dimensional autoregressive models such as PixelCNN \cite{oord2016conditional, salimans2017pixelcnn, chen2018pixelsnail} and WaveNet \cite{oord2016wavenet} are estimated to obtain a complex prior distribution. Joint training of the prior and the VQ-VAE is a challenging task for which no satisfactory solutions exist yet. Our work addresses both problems by introducing a new mathematical framework that extends and generalizes the standard VQ-VAE. Our method enables end-to-end training and, in particular, bypasses the separate training of an autoregressive prior.

%\paragraph{Priors in VQ-VAEs}
An autoregressive pixelCNN prior model has several drawbacks, which are the same in the pixel space or in the latent space.  The data is assumed to have a fixed sequential order, which forces the generation to start at a certain point, typically in the upper left corner, and span the image or the 2-dimensional latent lattice in an arbitrary way. At each step, a new latent variable is sampled using the previously sampled pixels or latent variables. Inference may then accumulate prediction errors, while training provides ground truth at each step. % (“teacher-forcing”).
 The runtime process, which depends mainly on the number of network evaluations, is sequential and depends on the size of the image or the 2-dimensional latent lattice, which can become very large for high-dimensional objects.
 
 The influence of the prior is further explored in \cite{razavi2019generating}, where VQ-VAE is used to sample images on a larger scale, using two layers of discrete latent variables, and  \cite{willetts:2021} use hierarchical discrete VAEs with numerous layers of latent variables. Other works such as \cite{esser2021taming, ramesh2021zero} have used Transformers to autoregressively model a sequence of latent variables: while these works benefit from the recent advances of Transformers for large language models, their autoregressive process still suffers from the same drawbacks as pixelCNN-like priors. Departing from autoregressive models, our contributions are summarized as follows.
 
\begin{itemize}
    \item We propose a new mathematical framework for VQ-VAEs. We introduce a two-stage prior distribution. Following the diffusion probabilistic model approach of \cite{ho2020denoising}, we consider first a continuous latent vector parameterised as a Markov chain. The discrete latent states are defined  as random functions of this Markov chain. The  transition kernels of the continuous latent variables are trained using diffusion bridges to gradually produce samples that match the data.
    %\item Diffusion bridges are proposed 
    \item  To our best knowledge, this is the first probabilistic generative model to use denoising diffusion in discrete latent space. This framework allows for end-to-end training of VQ-VAE.
    \item We present our method on a toy dataset and then compare its efficiency to the pixelCNN prior of the original VQ-VAE on the miniImagenet dataset.
\end{itemize}
Figure~\ref{fig:archi} describes the complete architecture of our model.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{images/Archi.png}
    \caption{Our proposed architecture, for a prior based on a Ornstein-Uhlenbeck bridge. The top pathway from \textit{input image} to $\latentcont^0$, to $\latentdis^0$, to \textit{reconstructed image} resembles the original VQ-VAE model. The vertical pathway from $(\latentcont^0, \latentdis^0)$ to $(\latentcont^T, \latentdis^T)$ and backwards is based on a denoising diffusion process. See Section~\ref{sec:OU} and Algorithm~\ref{alg:sample} for the corresponding sampling procedure.}
    \label{fig:archi}
\end{figure}
\section{Related Works}

\paragraph{Diffusion Probabilistic Models.}
A promising class of models that depart from autoregressive models are Diffusion Probabilistic Models \cite{sohldickstein2015deep, ho2020denoising} and closely related Score-Matching Generative Models \cite{song2019generative, de2021simulating}. The general idea is to apply a corrupting Markovian process on the data through $T$ corrupting steps and learn a neural network that gradually \textit{denoises} or reconstructs the original samples from the noisy data.
For example, when sampling images, an initial sample is drawn from an uninformative distribution and reconstructed iteratively using the trained Markov kernel. This process is applied to all pixels simultaneously, so no fixed order is required and the sampling time does not depend on sequential predictions that depend on the number of pixels, but on the number of steps $T$. While this number of steps can be large ($T=1000$ is typical), simple improvements enable to reduce it dramatically and obtain $\times 50$ speedups \cite{song2021denoising}. These properties have led diffusion probability models to receive much attention in the context of continuous input modelling.


\paragraph{From Continuous to Discrete Generative denoising.}
In \cite{hoogeboom2021argmax}, the authors propose multinomial diffusion to gradually add categorical noise to discrete samples for which the generative denoising process is learned. Unlike alternatives such as normalizing flows, the diffusion proposed by the authors for discrete variables does not require gradient approximations because the parameter of the diffusion is fixed.

Such diffusion models are optimized using variational inference to learn the denoising process, i.e., the bridge that aims at inverting the multinomial diffusion. In \cite{hoogeboom2021argmax}, the authors propose a variational distribution based on bridge sampling.  %\tcr{i.e. sampling conditional on the initial samples. This is a major weakness, because if we assume that the model $p_{\theta}$ is a Markov chain, the process that evolves backwards in time is also a Markov chain... Eric: useful ? difficult to understand... point to the supplement}
In \cite{austin2021structured}, the authors improve the idea by modifying the transition matrices of the corruption scheme with several tricks. The main one is the addition of absorbing states in the corruption scheme by replacing a discrete value with a MASK class, inspired by recent Masked Language Models like BERT. In this way, the corrupted dimensions can be distinguished from the original ones instead of being uniformly sampled. 
%ii) Use of transition matrices that mimic Gaussian kernels in continuous space by imitating a diffusion model in continuous space through the use of a discretised truncated Gaussian distribution. However, this assumes a natural ordering of the dimensions, which is not the case in our latent space. iii)  A corruption based on nearest neighbors in the embedding space using previously trained embeddings of discrete categories. This scheme was developed for token embeddings such as pre-trained word embeddings and is not applicable in our case. 
One drawback of their approach, mentioned by the authors, is that the transition matrix does not scale well for a large number of embedding vectors, which is typically the case in VQ-VAE.

Compared to discrete generative denoising, our approach takes advantage of the fact that the discrete distribution depends solely on a continuous distribution in VQ-VAE. We derive a novel model based on continuous-discrete diffusion that we believe is simpler and more scalable than the models mentioned in this section.

\paragraph{From Data to Latent Generative denoising.}
Instead of modelling the data directly, \cite{vahdat2021score} propose to perform score matching in a latent space. The authors propose a complete generative model and are able to train the encoder/decoder and score matching end-to-end. Their method also achieve excellent visual patterns and results but relies on a number of optimization heuristics necessary for stable training. In \cite{mittal2021symbolic}, the authors  have also applied such an idea in a generative music model. Instead of working in a continuous latent space, our method is specifically designed for a discrete latent space as in VQ-VAEs.


\paragraph{Using Generative denoising in discrete latent space. }
In the model proposed by \cite{gu2021vector}, the autoregressive prior is replaced by a discrete generative denoising process, which is perhaps closer to our idea. However, the authors focus more on a text-image synthesis task where the generative denoising model is traine based on an input text: it generates a set of discrete visual tokens given a sequence of text tokens. They also consider the VQ-VAE as a trained model and focus only on the generation of latent variables. This work focuses instead on deriving a full generative model with a sound probabilistic interpretation that allows it to be trained end-to-end.

\section{Diffusion bridges VQ-VAE}
 \subsection{Model and loss function}
 Assume that the distribution of the input $x\in\rset^m$ depends on a hidden discrete state $\latentdis\in\embedspace = \{\embed_1,\ldots,\embed_K\}$ with $\embed_k\in\rset^d$ for all $1\leqslant k \leqslant K$. Let $p_\theta$ be the joint probability density of $(\latentdis,x)$%: for all $(\latentdis,x)\in\embedspace\times\rset^m$,
 $$
 (\latentdis,x)\mapsto p_\theta(\latentdis,x) =  p_\theta(\latentdis) p_\theta(x|\latentdis)\,,
 $$
 where $\theta\in\rset^p$ are unknown parameters.
Consider first an encoding function $f_\varphi$ and write $\latentcont(x)= f_\varphi(x)$ the encoded data. In the original VQ-VAE, the authors proposed the following variational distribution to approximate $p_\theta(\latentdis|x)$:
 $$
  q_\varphi(\latentdis|x) = \delta_{\embed_{k^*_x}}(\latentdis)\,,
  $$
  where $\delta$ is the Dirac mass and
 $$
  k^*_x = \mathrm{argmin}_{1\leqslant k \leqslant K}\left\{\|\latentcont(x)-\embed_k\|_2\right\}\,,
 $$
  where $\varphi\in\rset^r$ are all the variational parameters.
%  This approach  does not suffer from usual posterior collapse in VAE setting but involves many practical tricks and still suffer from serious limitations, see \cite{}. First, the authors proposed a rough approximation of the gradient of the loss function by copying gradients from the decoder input  to the
% encoder output. Second, the prior distribution $p_\theta(\latentdis)$ is first assumed to be uniform during training of the VQ-VAE. However,  very high dimensional autoregressive models such as PixelCNN \cite{} or WaveNet \cite{} are then trained to obtain a complex prior distribution during a second phase of training. Training the prior and the VQ-VAE jointly is a challenging task that has not yet received satisfactory solutions.

In this paper, we introduce a diffusion-based generative VQ-VAE. This model allows to propose a VAE approach with an efficient joint training of the prior and the variational approximation. %Diffusion-based generative models have recently shown striking results in image generation and can be used in our setting to define noising and denoising processes in a continuous state space to sample sequences of discrete representations. 
Assume that $\latentdis$ is a sequence, i.e.  $\latentdis= \latentdis^{0:T}$, where for all sequences $(a_u)_{u\geqslant 0}$ and all $0\leqslant s\leqslant t$, $a^{s:t}$ stands for $(a_s,\ldots,a_t)$. Consider the following joint probability distribution 
$$
p_{\theta}(\latentdis^{0:T},x) = p^{\latentdis}_{\theta}(\latentdis^{0:T})p^x_{\theta}(x|\latentdis^{0})\,.
$$ 
The latent discrete state $\latentdis^0$ used as input in the decoder  is the final state of the chain $(\latentdis^T,\ldots,\latentdis^0)$. We further assume that $p_{\theta}^{\latentdis}(\latentdis^{0:T})$ is the marginal distribution of  
\begin{multline*}
p_{\theta}(\latentdis^{0:T},\latentcont^{0:T}) %&= p_{\theta,T}(Z_q^T,Z_e^T) \left\{\prod_{t=0}^{T-1}p_{\theta,t}(Z_q^t,Z_e^t|Z_q^{t+1},Z_e^{t+1})\right\}  p_{\theta,0}(X|Z^{0:T}_q,Z_e^{0:T})\,,\end{multline*}
= p^{\latentcont}_{\theta,T}(\latentcont^T) p^{\latentdis}_{\theta,T}(\latentdis^T|\latentcont^T)\\
\times\prod_{t=0}^{T-1}p^{\latentcont}_{\theta,t|t+1}(\latentcont^t|\latentcont^{t+1})p^{\latentdis}_{\theta,t}(\latentdis^t|\latentcont^t)\,.
\end{multline*}
%$$
%p_{\theta}(Z_q^{0:T},Z_e^{0:T}, X) %&= p_{\theta,T}(Z_q^T,Z_e^T) \left\{\prod_{t=0}^{T-1}p_{\theta,t}(Z_q^t,Z_e^t|Z_q^{t+1},Z_e^{t+1})\right\}  p_{\theta,0}(X|Z^{0:T}_q,Z_e^{0:T})\,,\\
%= p^u_{\theta,T}(Z_e^T) p^z_{\theta,T}(Z_q^T|Z_e^T)\left\{\prod_{t=0}^{T-1}p^u_{\theta,t}(Z_e^t|Z_e^{t+1})p^z_{\theta,t}(Z_q^t|Z_e^t)\right\}p^x_{\theta,0}(X|Z^{0}_q)\,.
%$$
In this setting, $\{\latentcont^t\}_{0\leqslant t\leqslant T}$ are continuous latent states in $\mathbb{R}^{d\times N}$ and conditionally on $\{\latentcont^t\}_{0\leqslant t\leqslant T}$ the $\{\latentdis^t\}_{0\leqslant t\leqslant T}$ are independent with discrete distribution with support $\embedspace^N$. This means that we model jointly $N$  latent states as this is useful for many applications such as image generation. %Therefore, the bivariate process $\{(\latentcont^t,\latentdis^t)\}_{0\leqslant t\leqslant T}$  follows a Hidden Markov model (HMM). 
The continuous latent state is assumed to be a Markov chain and at each time step $t$ the discrete variable $\latentdis^t$ is a random function of the corresponding $\latentcont^t$.  Although the continuous states are modeled as a Markov chain, the discrete variables arising therefrom have a more complex statistical structure (and in particular are not Markovian).

The prior distribution of  $\latentcont^T$ is assumed to be uninformative and this is the sequence of denoising transition densities $\{p^{\latentcont}_{\theta,t|t+1}\}_{0\leqslant t\leqslant T-1}$ which provides the final latent state $\latentcont^0$ which is mapped to the embedding space and used in the decoder, i.e. the conditional law of the data given the latent states. The final discrete $\latentdis^0$ only depends the continuous latent variable  $\latentcont^0$, similar to the dependency between $\latentdis$ and $\latentcont$ in the original VQ-VAE.

%Most approaches \cite{} using diffusion probabilistic models for $\{\latentdis^t\}_{T\leqslant t\leqslant 0}$ are based on a fixed diffusion process. A notable exception is \cite{} where the authors learned a Gaussian diffusion process. This specific choice was motivated by the design of the generative model which required to "invert" the diffusion process, i.e. to sample from a bridge process associated with Stochastic Differential Equation (SDE), which is explicit only for very few classes of diffusions.

Since the conditional law $p_{\theta}(\latentdis^{0:T},\latentcont^{0:T}| x)$ is not available explicitly, this work focuses on  variational approaches to provide an approximation. Then, consider the following variational family:
\begin{multline*}
q_{\varphi}(\latentdis^{0:T},\latentcont^{0:T}| x) = \delta_{\latentcont(x)}(\latentcont^0)q_{\varphi,0}^{\latentdis}(\latentdis^0|\latentcont^0)\\
\times\prod_{t=1}^T\left\{ q^{\latentcont}_{\varphi,t|t-1}(\latentcont^t|\latentcont^{t-1})q^{\latentdis}_{\varphi,t}(\latentdis^t|\latentcont^t)\right\}\,.
\end{multline*}
The family $\{q^{\latentcont}_{\varphi,t|t-1}\}_{1\leqslant t \leqslant T}$  of forward "noising" transition densities are chosen to be the transition densities of a continuous-time process $(Z_t)_{t\geqslant 0}$ with $Z_0 = \latentcont(x)$. Sampling the diffusion bridge $(\tilde Z_t)_{t\geqslant 0}$, i.e. the law of the process $(Z_t)_{t\geqslant 0}$  conditioned on $Z_0 = \latentcont(x)$ and $Z_T = \latentcont^T$ is a challenging problem for general diffusions, see for instance \cite{beskos2008mcmc,lin2010generating,bladt2016simulation}. By the Markov property, the  marginal density at time $t$ of this conditioned process is given by:
\begin{equation}
\label{eq:markov:bridge}
\bckw^{\latentcont}_{\varphi,t|0,T}(\latentcont^t|\latentcont^0,\latentcont^T) = \frac{q^{\latentcont}_{\varphi,t|0}(\latentcont^t|\latentcont^{0})q^{\latentcont}_{\varphi,T|t}(\latentcont^T|\latentcont^{t})}{q^{\latentcont}_{\varphi,T|0}(\latentcont^T|\latentcont^{0})}\,.
\end{equation}
The Evidence Lower BOund (ELBO) is then defined, for all $(\theta,\varphi)$, as
$$
\mathcal{L}(\theta,\varphi) = \mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta}(\latentdis^{0:T},\latentcont^{0:T},x)}{q_{\varphi}(\latentdis^{0:T},\latentcont^{0:T}| x)}\right]\,,
$$
where $\mathbb{E}_{q_{\varphi}}$ is the expectation under $q_{\varphi}(\latentdis^{0:T},\latentcont^{0:T}| x)$.
\begin{lemma}
\label{lem:loss}
For all $(\theta,\varphi)$, the ELBO $\mathcal{L}(\theta,\varphi)$ is:
\begin{multline*}
\mathcal{L}(\theta,\varphi) = \mathbb{E}_{q_{\varphi}}\left[\log p^x_{\theta}(x|\latentdis^{0})\right] + \sum_{t=0}^T \mathcal{L}_t(\theta,\varphi)\\
+ \sum_{t=0}^T\mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}{q_{\varphi,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}\right]\,, 
\end{multline*}
where, for $1\leqslant t \leqslant T-1$,
\begin{align*}
\mathcal{L}_0(\theta,\varphi) &=  \mathbb{E}_{q_\varphi}\left[\log p^{\latentcont}_{\theta, 0|1}(\latentcont^0|\latentcont^{1})\right]\,,\\
\mathcal{L}_t(\theta,\varphi) &= \mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta,t-1|t}^{\latentcont}(\latentcont^{t-1}|\latentcont^{t})}{q^{\latentcont}_{\varphi,t-1|0,t}(\latentcont^{t-1}|\latentcont^{0},\latentcont^{t})}\right]\,,\\
    \mathcal{L}_T(\theta,\varphi) &= \mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta,T}(\latentcont^{T})}{q_{\varphi,T|0}^{\latentcont}(\latentcont^{T}|\latentcont^{0})}\right]\,.
\end{align*}
\end{lemma}
\begin{proof}
The proof is standard and postponed to Appendix~\ref{ap:loss}.
\end{proof}
The three terms of the objective function can be interpreted as follows:
$$
\mathcal{L}(\theta,\varphi) = \mathcal{L}^{rec}(\theta,\varphi) + \sum_{t=0}^T \mathcal{L}_t(\theta,\varphi) + \sum_{t=0}^T \mathcal{L}^{reg}_t(\theta,\varphi)
$$
with $\mathcal{L}^{rec} = \mathbb{E}_{q_{\varphi}}[\log p^x_{\theta}(x|\latentdis^{0})]$ a reconstruction term, $\mathcal{L}_t$ the diffusion term, and an extra term \begin{equation}
\mathcal{L}^{reg}_t = \mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}{q_{\varphi,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}\right]\,,
\end{equation}
which may be seen as a regularization term as discussed in next sections.
% Noising and denoising over the random variables $\{\latentcont^t\}_{0\leqslant t\leqslant T}$ is sensitive to the initial scaling of the distance between embedded data and embeddings in $\embedspace$. An appealing solution to avoid this problem is to normalize all the distances and use for instance spherical projection. 
\subsection{Application to Ornstein-Uhlenbeck processes}
\label{sec:OU}
%\paragraph{Application to Ornstein-Uhlenbeck processes.}
Consider for instance the following Stochastic Differential Equation (SDE) to add noise to the  normalized inputs:
\begin{equation}
\label{eq:ou}
\mathrm{d}Z_t = -\vartheta (Z_t - z_*)\mathrm{d}t + \eta\mathrm{d}W_t\,,
\end{equation}
where $\vartheta, \eta>0$,  $z_*\in\mathbb{R}^{d\times N}$ is the target state at the end of the noising process and $\{W_t\}_{0\leqslant t\leqslant T}$ is a standard Brownian motion in $\mathbb{R}^{d\times N}$. We can define the variational density by integrating this SDE along small step-sizes. Let $\delta_t$ be the time step between the two consecutive latent variables $\latentcont^{t-1}$ and $\latentcont^{t}$. In this setting, $q^{\latentcont}_{\varphi,t|t-1}(\latentcont^t|\latentcont^{t-1})$ is a Gaussian probability density function with mean $z_* + (\latentcont^{t-1}-z_*)\mathrm{e}^{-\vartheta \delta_t}$ in $\mathbb{R}^{d\times N}$ and covariance matrix $(2\vartheta)^{-1}\eta^2(1-\mathrm{e}^{-2\vartheta\delta_t})\mathbf{I}_{dN}$, where for all $n\geqslant 1$, $\mathbf{I}_{n}$ is the identity matrix with size $n\times n$. Asymptotically the process is a Gaussian with mean $z_*$ and variance $\eta^2(2\vartheta)^{-1} \mathbf{I}_{dN}$.

The denoising process amounts then to sampling from the bridge associated with the SDE, i.e. sampling $\latentcont^{t-1}$ given $\latentcont^0$ and $\latentcont^t$. The law of this bridge is explicit for the Ornstein-Uhlenbeck diffusion \eqref{eq:ou}.
Using \eqref{eq:markov:bridge},
$$
\bckw^{\latentcont}_{\varphi,s|0,t}(\latentcont^{s}|\latentcont^{t},\latentcont^{0}) \propto q^{\latentcont}_{\varphi,s|0}(\latentcont^{t-1}|\latentcont^{0}) q^{\latentcont}_{\varphi,t|s}(\latentcont^{t}|\latentcont^{s})\,,
$$
where $0\leqslant s\leqslant t$, so that $\bckw^{\latentcont}_{\varphi,t-1|0,t}(\latentcont^{t-1}|\latentcont^{t},\latentcont^{0})$ is a Gaussian probability density function with mean
% \begin{multline*}
% \tilde \mu_{t|t-1,0} = \frac{\sigma^2_{t|t-1,0}}{v_{t-1}}\left(\latentcont^0\mathrm{e}^{-\theta\delta(t-1)}+z_*(1-\mathrm{e}^{-\theta\delta(t-1)})\right)\\  + \frac{\sigma^2_{t|t-1,0}}{v_{1}}\mathrm{e}^{-\theta\delta}\left(\latentcont^t - z_*(1 - \mathrm{e}^{-\theta\delta} )\right)
% \end{multline*}
% and variance
% $$
% \tilde \sigma^2_{t|t-1,0} = \left(\frac{1}{v_{t-1}} + \frac{\mathrm{e}^{-2\theta\delta}}{v_1}\right)^{-1}\,.
% $$
\begin{multline*}
 \tilde \mu_{\varphi,t-1|0,t}(\latentcont^0,\latentcont^t) = \frac{\beta_t}{1-\bar{\alpha}_t}\left(z_* + \sqrt{\bar{\alpha}_{t-1}}(\latentcont^0-z_*)\right)\\  + \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\sqrt{\alpha}_t\left(\latentcont^t - (1 - \sqrt{\alpha_t} )z_*\right)
 \end{multline*}
 and covariance matrix
 $$
 \tilde \sigma^2_{\varphi,t-1|0,t} = \frac{\eta^2}{2\vartheta}\frac{1- \bar \alpha_{t-1}}{1- \bar \alpha_{t}}\beta_t\, \mathbf{I}_{dN}\,,
 $$
 where $\beta_t = 1 - \mathrm{exp}(-2\vartheta \delta_t)$, $\alpha_t = 1-\beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^{t}\alpha_s$.
 Note that the bridge sampler proposed in \cite{ho2020denoising} is a specific case of this setting with $\eta = \sqrt{2}$, $z_*=0$ and $\vartheta = 1$. 
% Let $\pi$ be a probability distribution function with respect to the Lebesgue measure such that $\pi \propto \mathrm{exp}(-G)$ where $G$ is continuously differentiable.
% We consider a sampling method based on the Euler discretization of the overdamped Langevin
% stochastic differential equation (SDE):
% $$
% \mathrm{d}U_t = -\nabla G(U_t)\mathrm{d}t + \sqrt{2}\mathrm{d}W_t\,.
% $$
% Under standard assumptions, see \cite{}, the convergence to $\pi$ holds at geometric rate. In this case, $\pi$ is the equilibrium distribution, and our model will target this distribution as initial law of $Z_e^T$

\paragraph{Choice of denoising model $p_\theta$.}
Following \cite{ho2020denoising}, we propose a Gaussian distribution for $p_{\theta,t-1|t}^{\latentcont}(\latentcont^{t-1}|\latentcont^{t})$ with mean $\mu_{\theta,t-1|t}(\latentcont^{t},t)$ and variance $\sigma_{\theta,t|t-1}^2\,\mathbf{I}_{dN}$. In the following, we choose $$
\sigma_{\theta,t|t-1}^2 = \frac{\eta^2}{2\vartheta}\frac{1- \bar \alpha_{t-1}}{1- \bar \alpha_{t}}\beta_t\,
$$
so that the term $\mathcal{L}_t$ of Lemma~\ref{lem:loss} writes
\begin{multline*}
    2\sigma_{\theta,t|t-1}^2\mathcal{L}_t(\theta,\varphi) \\
    = -\mathbb{E}_{q_\varphi}\left[\left\|\mu_{\theta,t-1|t}(\latentcont^{t},t) -  \tilde \mu_{\varphi,t-1|0,t}(\latentcont^0,\latentcont^t)\right\|_2^2\right]\,.
\end{multline*}
In addition, under $q_\varphi$, $\latentcont^t$ has the same distribution as
$$
\latentcontpred^t(\latentcont^{0},\varepsilon_t) = z_* + \sqrt{\bar \alpha_t}(\latentcont^{0}-z_*) + \sqrt{\frac{\eta^2}{2\vartheta}(1-\bar\alpha_t)}\varepsilon_t\,,
$$
where $\varepsilon_t \sim \mathcal{N}(0,\mathbf{I}_{dN})$. 
% we can reparametrise the model to predict the noise, given that $Z_e^{t}=Z_e^{0}\mathrm{e}^{-\theta\delta t} + \sigma_{t} \varepsilon$ . (adapt with $z*$)
% \begin{equation}
%     Z_e^{0}=(Z_e^{t} - \sigma_{t}\varepsilon) \mathrm{e}^{\theta\delta t} 
% \end{equation}
% \begin{align*}
% q^u_{\varphi,t}(Z_e^{t-1}|Z_e^{t},Z_e^{0})=\mathcal{N}(\tilde \mu_{t|t-1,0} (Z_e^{t},Z_e^{0}),\,\sigma^{2})\\
% p^u_{\theta,t}(Z_e^{t-1}|Z_e^t)=\mathcal{N}( \mu_{\theta,t|t-1,0} (Z_e^{t},t),\,\sigma^{2})
% \end{align*}
Then, for instance in the case $z_*=0$, $\tilde \mu_{\varphi,t-1|0,t}$ can be reparameterised as follows:
\begin{multline*}
    \tilde \mu_{\varphi,t-1|0,t}(\latentcont^0,\latentcont^t) =\\% - \sqrt{\alpha_t}\frac{1-\bar \alpha_{t-1}}{1-\bar \alpha_t}z_* \\ 
     \frac{1}{\sqrt{\alpha_t}}\left(\latentcontpred^t(\latentcont^0,\varepsilon_t) - \sqrt{\frac{\eta^2}{2\vartheta (1-\bar{\alpha}_t)}}\beta_t\varepsilon_t\right)\,.
\end{multline*}
We therefore propose to use
\begin{multline*}
    \mu_{\theta,t-1|t}(\latentcont^{t},t) =\\% - \sqrt{\alpha_t}\frac{1-\bar \alpha_{t-1}}{1-\bar \alpha_t}z_* \\ 
     \frac{1}{\sqrt{\alpha_t}}\left(\latentcont^t - \sqrt{\frac{\eta^2}{2\vartheta (1-\bar{\alpha}_t)}}\beta_t\varepsilon_\theta(\latentcont^{t},t)\right)\,,
\end{multline*}
which yields
\begin{equation}
    \label{eq:hoparam}
    \mathcal{L}_t(\theta,\varphi) \\
    = \frac{-\beta_t}{2\alpha_t(1-\bar\alpha_t)}\mathbb{E}\left[\left\|\varepsilon_t - \varepsilon_\theta(\latentcontpred^t(\latentcont^{0},\varepsilon_t) ,t) \right\|_2^2\right]\,.
\end{equation}
    
Several choices can be proposed to model the function $\varepsilon_\theta$. The deep learning architectures considered in the numerical experiments are discussed in Appendix~\ref{ap:networks} and ~\ref{ap:additionaltoy}. Similarly to \cite{ho2020denoising}, we use a stochastic version of our loss function:  sample $t$ uniformly in $\{0, \ldots,  T\}$, and consider $\mathcal{L}_t(\theta,\varphi)$ instead of the full sum over all $t$. The final training algorithm is described in Algorithm~\ref{alg:train} and the sampling procedure in Algorithm~\ref{alg:sample}. 

% The mean is parameterized as follows.
% \begin{equation}
%     \mu_\theta(\latentcont^{t},t)=\tilde \mu_{t}((\latentcont^{t} - \sigma_{t}\boldsymbol{\varepsilon_{\theta}(\latentcont^{t})}) \mathrm{e}^{\theta\delta t},\latentcont^{t})
% \end{equation}

% To sample from $p^u_{\theta,t}(Z_e^{t-1}|Z_e^t)$, we predict the noise $\boldsymbol{\varepsilon_{\theta}}$ first. 


%\paragraph{Extension to more complex bridge processes.}

%\paragraph{Discussion about score-based generative modeling.}

% \medskip

% Insert a graphical explanation of the full model, with encoding, decoding, and diffusion process \todo{anyone} 

\paragraph{Connections with the VQ-VAE loss function. } In the special case where $T=0$, our loss function can be reduced to a standard VQ-VAE loss function. In that case, write $\latentdis = \latentdis^0$ and $\latentcont = \latentcont^0$, the ELBO then becomes: 
$$
\mathcal{L}(\theta,\varphi) = \mathbb{E}_{q_{\varphi}}\left[\log p^x_{\theta}(x|\latentdis)\right]
+ \mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta}^{\latentdis}(\latentdis|\latentcont)}{q_{\varphi}^{\latentdis}(\latentdis|\latentcont)}\right]\,, 
$$
Then, if we assume that $p_{\theta}^{\latentdis}(\latentdis|\latentcont) = \mathrm{Softmax}\{-\|\latentcont - \embed_k\|^2_2\}_{1\leq k \leq K}$ and that  $q_{\varphi}^{\latentdis}(\latentdis|\latentcont)$ is as in \cite{oord2017neural}, i.e. a Dirac mass at $\widehat{\latentdis} = \mathrm{argmin}_{1\leq k \leq K}\|\latentcont - \embed_k\|^2_2$, up to an additive constant, this yields the following random estimation of $\mathbb{E}_{q_{\varphi}}[\log p_{\theta}^{\latentdis}(\latentdis|\latentcont)/q_{\varphi}^{\latentdis}(\latentdis|\latentcont)]$,
% \begin{equation}
% \label{eq:lossvqvae}
% \mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta}^{\latentdis}(\latentdis|\latentcont)}{q_{\varphi}^{\latentdis}(\latentdis|\latentcont)}\right] = -\mathbb{E}_{q_{\varphi}}\left[\log q_{\varphi}^{\latentdis}(\latentdis|\latentcont)\right]\,.
% \end{equation}
% Choosing $q_{\varphi}^{\latentdis}(\latentdis|\latentcont) = \mathrm{Softmax}\{-\|\latentcont - \embed_k\|_2\}_{1\leq k \leq K}$ and sampling $\latentdis \sim q_{\varphi}^{\latentdis}(\latentdis|\latentcont)$ yields the following random estimation of \eqref{eq:lossvqvae},
\begin{multline*}
    \widehat{\mathcal{L}}^{reg}_{\latentdis}(\theta,\varphi) = \|\latentcont - \widehat{\latentdis}\|_2 \\+ \log \left(\sum_{k=1}^{K}\exp\left\{-\|\latentcont-\embed_k\|_2\right\}\right)\,.
\end{multline*}
The first term of this loss is the loss proposed in \cite{oord2017neural} which is then split into two parts using the stop gradient operator. The last term is simply the additional normalizing term of  $p_{\theta}^{\latentdis}(\latentdis|\latentcont)$. % which is not considered in \cite{oord2017neural}. % and is useless if no optimization on the embeddings is performed. 
%Therefore, our setting encompasses the standard VQ-VAE setting.% without assuming during training that $p_{\theta}^{\latentdis}(\latentdis|\latentcont)$ is uniform. 

\paragraph{Connecting diffusion and discretisation. } Similar to the VQ-VAE case above, it is possible to consider only the term $\mathcal{L}^{reg}_0(\theta,\varphi)$ in the case $T > 0$. However, our framework allows for much flexible parameterisation of $p_{\theta,t}^{\latentdis}(\latentdis^t|\latentcont^t)$ and $q_{\varphi,t}^{\latentdis}(\latentdis^t|\latentcont^t)$. For instance, the Gumbel-Softmax trick provides an efficient and differentiable parameterisation. A sample $\latentdis^t\sim  p_{\theta,t}^{\latentdis}(\latentdis^t|\latentcont^t)$ (resp. $\latentdis^t\sim q_{\varphi,t}^{\latentdis}(\latentdis^t|\latentcont^t)$) can be obtained by sampling with probabilities proportional to $\{\exp\{(-\|\latentcont - \embed_k\|^2_2 + G_k )/\tau_t\}\}_{1\leq k \leq K}$ (resp. $\{\exp\{(-\|\latentcont - \embed_k\|^2_2 + \tilde G_k )/\tau\}\}_{1\leq k \leq K}$), where $\{(G_k,\tilde G_k)\}_{1\leq k \leq K}$ are i.i.d. with distribution $\mathrm{Gumbel}(0,1)$, $\tau>0$, and $\{\tau_t\}_{0\leq t \leq T}$ are positive  time-dependent scaling parameters. In practice, the third part of the objective function can be computed efficiently, by using a stochastic version of the ELBO, computing a single $\mathcal{L}^{reg}_t(\theta,\varphi)$ instead of the sum (we use the same $t$ for both parts of the ELBO). The term reduces to:
% (see Appendix~\ref{ap:reg} for details):
% \begin{equation} 
% \label{eq:reg}
% \mathcal{L}^{reg}_t(\theta,\varphi) = \mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}{q_{\varphi,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}\right]
% \approx \gamma_t \mathbb{E}_{q_{\varphi}}\left[\left\|\latentcont^{t} - \latentdis^t \right\|_2^2\right]\
% \end{equation}

\begin{equation}
    \mathcal{L}^{reg}_t(\theta,\varphi) = -\mathrm{KL}(q_\varphi(\latentdis^{t}|\latentcont^{t})\|p_\theta(\latentdis^{t}|\latentcont^{t}))\,. 
\end{equation}
% Where $\gamma_t$ is a weight term which depends on the temperatures chosen in the Gumbel-Softmax.
This terms connects the diffusion and quantisation parts as it creates a gradient pathway through a step $t$ of the diffusion process, acting as a regularisation on the codebooks and $\latentcont^t$. Intuitively, maximizing $\mathcal{L}^{reg}_t(\theta,\varphi)$  accounts for pushing codebooks and $\latentcont^t$ together or apart depending on the choice of $\tau, \tau_t$. The final end-to-end training algorithm is described in Algorithm~\ref{alg:train}, and further considerations are provided  in Appendix~\ref{ap:reg}.

% \paragraph{Connections with discrete diffusion.} Our model also generalizes a discrete diffusion, if we consider a special case with no encoder / decoder. The discrete data $X \in  \{1,\ldots,K\}^N$ is first mapped into (random) projection through the codebooks $\mathcal{E} = \{\rme_1,\ldots,\rme_K\}$, $K\geq 1$. The loss then becomes: 
% $$
% \mathcal{L}(\theta,\varphi) = \sum_{t=1}^T \mathcal{L}_t(\theta,\varphi)
% + \sum_{t=0}^T\mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}{q_{\varphi,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}\right]\,, 
% $$

% The first term here corresponds to a continuous diffusion process of the embedded vectors through the codebooks, and the second term can be seen as a regularization term which enables to learn the optimal codebooks alongside the diffusion process. Intuitively, we project the discrete variables in a continuous space in order to build a diffusion process which captures more efficiently the dependencies of the data.

\begin{algorithm}[tb]
   \caption{Training procedure}
   \label{alg:train}
\begin{algorithmic}
   \REPEAT
   %\STATE \COMMENT{\textit{Compute reconstruction loss}}
   

   \STATE Compute $\latentcont^0= f_\varphi(x)$ 
   \STATE Sample $\hat{\latentdis}^0 \sim q_\varphi(\latentdis^0|\latentcont^0)$
   \STATE Compute $\textcolor{teal}{\hat{\mathcal{L}}^{rec}(\theta,\varphi)} = \log p^x_{\theta}(x|\hat{\latentdis}^0)$ 
   %\STATE \COMMENT{\textit{Compute denoising loss}}

   \STATE Sample $t \sim Uniform(\{0,\ldots, T\})$ 
   \STATE Sample $\varepsilon_t \sim \mathcal{N}(0,\mathbf{I}_{dN})$
   \STATE Sample $\latentcont^{t} \sim q_\varphi(\latentcont^t|\latentcont^0)$ (using $\varepsilon_t$)
   \STATE Compute $\textcolor{olive}{\hat{\mathcal{L}}_t(\theta,\varphi)}$ from $\varepsilon_\theta(\latentcont^{t},t)$ and $\varepsilon_t$ using \eqref{eq:hoparam}
   \STATE Compute $\textcolor{violet}{\hat{\mathcal{L}}^{reg}_t(\theta,\varphi)}$ from $\latentcont^{t}$ \textit{(see text)}

   %\STATE $\epsilon \sim \mathcal{N}(0,\,\mathbf{I})$ 
   \STATE $\hat{\mathcal{L}}(\theta,\varphi) = \textcolor{teal}{\hat{\mathcal{L}}^{rec}(\theta,\varphi)} + \textcolor{olive}{\hat{\mathcal{L}}_t(\theta,\varphi)} + \textcolor{violet}{\hat{\mathcal{L}}^{reg}_t(\theta,\varphi)}$
   \STATE Perform SGD step on $-\hat{\mathcal{L}}(\theta,\varphi)$
   \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[tb]
   \caption{Sampling procedure (for $z_* = 0$)}
   \label{alg:sample}
\begin{algorithmic}
   
   \STATE Sample $\latentcont^T \sim \mathcal{N}(0, (2\vartheta)^{-1}\eta^2 \mathbf{I}_{dN})$ 
   \FOR{$t=T$ {\bfseries to} $1$}
   \STATE Set $\latentcont^{t-1} = \alpha_t^{-1/2}\left(\latentcont^t - \sqrt{\frac{\eta^2}{2\vartheta (1-\bar{\alpha}_t)}}\beta_t\varepsilon_\theta(\latentcont^{t},t)\right)$
   \ENDFOR
   \STATE  Sample $\latentdis^0 \sim  p^{\latentdis}_{\theta,0}(\latentdis^0|\latentcont^0)$ \COMMENT{\textit{quantisation}}
   \STATE Sample $x ~ \sim  p^x_{\theta}(x|\latentdis^0)$ \COMMENT{\textit{decoder}}
\end{algorithmic}
\end{algorithm}


\section{Experiments}

\subsection{Toy Experiment}
In order to understand the proposed denoising procedure for VQ-VAE, consider a simple toy setting in which there is no encoder nor decoder, and the codebooks $\{\embed_j\}_{0\leqslant j \leqslant K-1}$ are fixed. In this case, with $d=2$ and $N=5$, $x = \latentcont^0 \in \rset^{2\times 5}$. We choose $K=8$ and the codebooks $\embed_j = \mu_j \in \rset^2$, $0\leqslant j \leqslant K-1$, are fixed centers at regular angular intervals in $\rset^2$ and shown in Figure~\ref{fig:toydata}; the latent states $(\latentdis^t)_{1\leq t\leq T}$ lie in $\{\embed_0,\ldots,\embed_7\}^5$. Data generation proceeds as follows. First, sample a sequence of $(q_1,\ldots,q_5)$ in $\{0,\dots,7\}$: $q_1$ has a uniform distribution, and, for $s\in\{0,1,2,3\}$, $q_{s+1} = q_s + b_s \mod 8$, where $b_s$ are independent Bernoulli samples with parameter $1/2$ taking values in $\{-1, 1\}$. Conditionally on $(q_1,\ldots,q_5)$, $x$ is a Gaussian random vector with mean $(\embed_{q_1},\ldots,\embed_{q_5})$ and variance $\mathbf{I}_{2\times 5}$. 

% We consider the following data generation procedure. 
% \begin{itemize}
%     \item Sample a sequence of $q \in \{1,\dots,8\}^S$ of $S=5$ integers with temporal dependency: $q_1$ uniform, and $q_{s+1} = q_s + b_s \mod 8$, where $b_s$ are independant Bernouilli samples taking values in $\{-1, 1\}$ with probability $\frac{1}{2}$.
%     \item The function $p_\theta^x(x|\latentdis^0)$ is chosen as a Gaussian probability function with mean $\mu_{\latentdis^0}$ and variance $\mathbf{I}_2$.
%     %Sample $X_s \sim \mathcal{N}(\mu_{q_s},\,\mathbf{I}_2)$
% \end{itemize}

% , the codebooks $\mu_j = e_j \in \rset^2$ are fixed centres at regular angular intervals in a plane as in the 8 Gaussian toy dataset and shown in Fig \ref{fig:toydata}; $\latentdis$ is then the discrete state living in $\{\rme_1,\ldots,\rme_8\}$.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.45]{images/centroids2.png}
    \caption{Toy dataset, with $K=8$ centroids, and two samples $x = (x_1,x_2,x_3,x_4,x_5)$ in $\rset^{2 \times 5}$ each displayed as $5$ points in $\rset^{2}$ (blue and red points), corresponding to the discrete sequences (red) $(6,5,4,3,2)$ and (blue) $(7,0,1,0,1)$.}
    \label{fig:toydata}
\end{figure}

We train our bridge procedure with $T=50$ timesteps, $\vartheta=2, \eta=0.1$, other architecture details and the neural network $\varepsilon_\theta(\latentcont^t ,t)$ are described in Appendix~\ref{ap:additionaltoy}. Forward noise process and denoising using $\varepsilon_\theta(\latentcont^t ,t)$ are showcased in Figure~\ref{fig:noisedenoise}, and more illustrations and experiments can be found in Appendix~\ref{ap:additionaltoy}. 

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/noisedenoise2.png}
    \caption{(Left) Forward noise process for one sample. First, one data is drawn ($\latentcont^0(x) =x$ in the toy example)  and then $\{\latentcont^t\}_{1\leq t \leq T}$ are sampled under $q_\varphi$ and displayed. % corresponding to the discrete sequence $[7,0,1,2,3]$: 
    %$X^t_s$ is represented for $0 \leq t \leq 50$ and $s$ the displayed index. 
    (Right) Reverse process for one sample $\latentcont^T\sim  \mathcal{N}(0, (2\vartheta)^{-1}\eta^2 \mathbf{I}_{dN})$. As expected, the last sample $\latentcont^0$ reaches the neighborhood of $5$ codebooks.}
    %Generated samples have the same underlying discrete structure as the original generation procedure: the generated $X^0_s$ can be discretized by finding the nearest neighbour codebook into the following sequence $(5,6,5,4,3)$. Discrete sequences alongside the process can be found in Appendix~\ref{ap:discretetoy}} %It generates a valid sequence $[5,6,5,4,3]$.}
    \label{fig:noisedenoise}
\end{figure}

 
\paragraph{End-to-end training. } Contrary to VQ-VAE procedures in which the encoder/decoder/codebooks are trained separately from the prior, we can train the bridge prior alongside the codebooks. Consider a new setup, in which the $K=8$ codebooks are randomly initialized and considered as parameters of our model (they are no longer fixed to the centers of the data generation process $\mu_j$). The first part of our loss function, in conjunction with the Gumbel-Softmax trick makes it possible to train all the parameters of the model end-to-end. Details of the procedure and results are shown in Appendix~\ref{ap:end2end}.


% \paragraph{Selected Metrics}
% \begin{itemize}
%     \item NLL on the latent space
%     \item FID on images
%     \item precision/recall (cf charles)
%     \item evaluation of the perfs of the model
% \end{itemize}

\subsection{Image Synthesis}
In this section, we focus on image synthesis using CIFAR10 and miniImageNet datasets. The goal is to evaluate the efficiency and properties of our model compared to the original PixelCNN. Note that for fair comparisons, the encoder, decoder and codebooks are pretrained and fixed for all models, only the prior is trained and evaluated here. As our goal is the comparison of priors, we did not focus on building the most efficient VQ-VAE, but rather a reasonable model in terms of size and efficiency.

\paragraph{CIFAR10. }
 The CIFAR dataset consists of inputs $x$ of dimensions $32 \times 32$ with 3 channels. The encoder projects the input into a grid of continuous values $\latentcont^0$ of dimension $8 \times 8 \times 128$. After discretisation, $\{\latentdis^t\}_{0\leqslant t\leqslant T}$ are in a discrete latent space induced by the VQ-VAE which consists of values in $\{1,\ldots,K\}^{8 \times 8}$ with $K=256$. The pre-trained VQ-VAE reconstructions can be seen in Figure~\ref{fig:cifar_vqvae} in Appendix~\ref{ap:additional_visuals}.

\paragraph{miniImageNet. }
\textit{mini}ImageNet was introduced by \cite{Vinyals2016MatchingNF} to offer more complexity than CIFAR10, while still fitting in memory of modern machines.
600 images were sampled for 100 different classes from the original ImageNet dataset, then scaled down, to obtain 60,000 images of dimension $84 \times 84$.
In our experiments, we trained a VQVAE model to project those input images into a grid of continuous values $\latentcont^0$ of dimensions $21 \times 21 \times 32$, see Figure~\ref{fig:miniimagenet_vqvae} in Appendix~\ref{ap:additional_visuals}.
The associated codebook contains $K=128$ vectors of dimension $32$. 

\paragraph{Prior models. }
Once the VQ-VAE is trained on the miniImageNet (resp. CIFAR) dataset, the $84\times 84 \times 3$ (resp $32\times 32 \times 3$) images are passed to the encoder result in a $21 \times 21$ (resp. $8 \times 8$) feature map. From this model, we extract the discrete latent states from training samples to train a PixelCNN prior and the continuous latent states for our diffusion.
Concerning our diffusion prior, we choose the Ornstein-Uhlenbeck process setting $\eta = \sqrt{2}$, $z_*=0$ and $\vartheta = 1$, with $T=1000$. 

% We trained a PixelCNN as a prior on the latent space, as described in the original VQVAE paper. Sampling are displayed in Figure~\ref{fig:miniimagenet_prior_pixelcnn}

\subsection{Quantitative results}
We benchmarked our model using three metrics, in order to highlight the performances of the proposed prior, the quality of produced samples as well as the associated computation costs.
Results are given as a comparison to the original PixelCNN prior for both the \textit{mini}ImageNet (see Table \ref{tab:miniimagenet}) and the CIFAR10 (see Table~\ref{tab:cifar}) datasets.

\paragraph{NLL. }\label{nll_paragraph}
Unlike most related papers, we are interested in computing the Negative Log Likelihood (NLL) directly in the latent space, as to evaluate the capacity of the priors to generate coherent latent maps.
To this end, we mask a patch of the original latent space, and reconstruct the missing part, similar to image inpainting, following for instance \cite{van2016pixel}.
 In the case of our prior, for each sample $x$, we mask an area of the continuous latent state $\latentcont^0$, i.e. we mask some components of $\latentcont^0$, and aim at sampling the missing components given the observed ones using the prior model. Let $\underline{\latentdis}^0$ and $\underline{\latentcont}^0$ (resp. $\overline{\latentdis}^0$ and $\overline{\latentcont}^0$) be the masked (resp. observed) discrete and continuous latent variables. The target conditional likelihood is
\begin{align*}
p_{\theta}(\underline{\latentdis}^0|\overline{\latentcont}^0) &= \int p_{\theta}(\underline{\latentdis}^0,\underline{\latentcont}^0|\overline{\latentcont}^0)\mathrm{d} \underline{\latentcont}^0\,,\\
&= \int p_{\theta}(\underline{\latentdis}^0|\underline{\latentcont}^0)p_{\theta}(\underline{\latentcont}^0|\overline{\latentcont}^0)\mathrm{d} \underline{\latentcont}^0\,.
\end{align*}
This likelihood is intractable and replaced by a simple Monte Carlo estimate $\hat{p}_{\theta}(\underline{\latentdis}^0|\underline{\latentcont}^0)$ where $\underline{\latentcont}^0\sim p_{\theta}(\underline{\latentcont}^0|\overline{\latentcont}^0)$. Note that conditionally on $\underline{\latentcont}^0$ the components of $\underline{\latentdis}^0$ are assumed to be independent but $\underline{\latentcont}^0$ are sampled jointly under $p_{\theta}(\underline{\latentcont}^0|\overline{\latentcont}^0)$. As there are no continuous latent data in PixelCNN, $p_{\theta}(\underline{\latentdis}^0|\overline{\latentdis}^0)$ can be directly evaluated.

\paragraph{FID. }
We report Fr\'echet Inception Distance (FID) scores by sampling a latent discrete state $\latentdis \in \embedspace^N$ from the prior, and computing the associated image through the VQ-VAE decoder. In order to evaluate each prior independently from the encoder and decoder networks, these samples are compared to VQ-VAE reconstructions of the dataset images. 

\paragraph{Computation times. }
We evaluated the computation cost of sampling a batch of 32 images, on a GTX TITAN Xp GPU card. Note that the computational bottleneck of our model consists of the $T=1000$ sequential diffusion steps (rather than the encoder/decoder which is very fast). Therefore, a diffusion speeding technique such as the one described in \cite{song2021denoising} would be straightforward to apply and would likely provide a $\times 50$ speedup as mentioned in the paper. 

\begin{table}
    \caption{Results on \textit{mini}ImageNet. Metrics are computed on the validation dataset. The means are displayed along with the standard deviation in parenthesis.}
    \resizebox{.48\textwidth}{!}{\begin{tabular}{ |l||c|c|c| }
        \hline
        & NLL & FID & s/sample \\
        \hline
        PixelCNN \cite{oord2017neural}     & 1.00 ($\pm 0.05$)& 98 & 10.6s ($\pm 28ms$) \\
 %       cDiff (logits)                     & - & - & - \\  
 %       cDiff ($Z_e$)                      & - & - & - \\
 %       dDiff \cite{hoogeboom2021argmax}   & - & - & - \\
 %       D3PM \cite{austin2021structured}   & - & - & - \\
        Ours                               & 0.94 ($\pm 0.02$)& 99 & 1.7s ($\pm 10ms$)\\
        \hline
    \end{tabular}}
    \label{tab:miniimagenet}
\end{table}

\begin{table}
    \caption{Results on CIFAR10. Metrics are computed on the validation dataset. The means are displayed along with the standard deviation in parenthesis.}
    \resizebox{.48\textwidth}{!}{\begin{tabular}{ |l||c|c|c| }
        \hline
        & NLL & FID & s/sample \\
        \hline
        PixelCNN \cite{oord2017neural}     & 1.41 ($\pm 0.06)$ & 109 & 0.21 ($\pm 0.8ms$) \\
    %    cDiff (logits)                     & - & - & - \\
    %    cDiff ($Z_e$)                      & - & - & - \\
     %   dDiff \cite{hoogeboom2021argmax}   & - & - & - \\
     %   D3PM \cite{austin2021structured}   & - & - & - \\
        Ours                               & 1.33 ($\pm 0.18$)  &  104 & 0.05s ($\pm 0.5ms$) \\
        \hline
    \end{tabular}}
    \label{tab:cifar}
\end{table}


% \paragraph{Prior models}
% Once the VQ-VAE is trained on the miniImageNet dataset, the $84\times 84 \times 3$ images are passed to the encoder result in a $21 \times 21$ feature map. From this model, we extract the discrete latent states to train a PixelCNN prior and  the continuous latent states for our diffusion.
% Concerning our Diffusion prior, we choose the Ornstein-Uhlenbeck process setting $\eta = \sqrt{2}$, $z_*=0$ and $\vartheta = 1$, with $T=1000$
% and report visuals results on conditional and unconditional sampling from the prior. 

\subsection{Qualitative results}
\paragraph{Sampling from the prior. }
%The samples shown in this section depend on the performance of the VQ-VAE model, and for this reason we show the image reconstruction when necessary. 
Samples from the PixelCNN prior are shown in Figure \ref{fig:pixel_samples}
and samples from our prior in Figure \ref{fig:diffusion_samples}. Additional samples are given in Appendix~\ref{ap:additional_visuals}. Note that contrary to original VQ-VAE prior, the prior is not conditioned on a class, which makes the generation less specific and more difficult. However, the produced samples illustrate that our prior can generate a wide variety of images which show a large-scale spatial coherence in comparison with samples from PixelCNN. 


\begin{figure}[h!]
%\ContinuedFloat

\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=1.\linewidth]{images/ours/samples2.png}
    \caption{Samples from our diffusion prior.}
    \label{fig:diffusion_samples}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=1.\linewidth]{images/ours/pixel_samples.png}
    \caption{Samples from the PixelCNN prior.}
    \label{fig:pixel_samples}
\end{subfigure}
\caption{Comparison between samples from our diffusion-based prior (top) and PixelCNN prior (bottom).}
\label{fig:figures}
\end{figure}




\paragraph{Conditional sampling. }
As explained in Section~\ref{nll_paragraph},  for each sample $x$,  we mask some components of $\latentcont^0(x)$, and aim at sampling the missing components given the observed ones using the prior models. This conditional denoising process is further explained for our model in Appendix~\ref{ap:inpainting}. To illustrate this setting, we show different conditional samples for 3 images in Figure~\ref{fig:miniimagenet_prior_ours_conditional} and  Figure~\ref{fig:miniimagenet_prior_ours_conditional:topleft}  for both the PixelCNN prior and ours. In Figure~\ref{fig:miniimagenet_prior_ours_conditional},  the mask corresponds to a $9\times 9$ centered square over the $21\times 21$ feature map. In Figure~\ref{fig:miniimagenet_prior_ours_conditional:topleft},  the mask corresponds to a $9\times 9$ top left square. These figures illustrate that our diffusion model is much less sensitive to the selected masked region than PixelCNN. This may be explained by the use of our denoising function $\varepsilon_\theta$ which depends on all conditioning pixels while PixelCNN uses a hierarchy of masked convolutions to enforce
a specific conditioning order. Additional conditional sampling experiments are given in Appendix~\ref{ap:additional_visuals}.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/ours/cond_samples_center_2.png}

    \caption{Conditional sampling with centered mask: for each of the 3 different images, samples from our diffusion are on top and from PixelCNN on the bottom. For each row: the image on the left is the VQVAE masked reconstruction, the image on the right is the full VQ-VAE reconstruction. Images in-between are independent conditional samples from the models.}
    \label{fig:miniimagenet_prior_ours_conditional}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/ours/cond_samples_topleft.png}

    \caption{Conditional sampling with top left mask: for each of the 3 different images, samples from our diffusion are on top and from PixelCNN on the bottom. For each row: the image on the left is the VQVAE masked reconstruction, the image on the right is the full VQ-VAE reconstruction. Images in-between are independent conditional samples from the models.}
    \label{fig:miniimagenet_prior_ours_conditional:topleft}
\end{figure}


\paragraph{Denoising chain. }
In addition to the conditional samples, Figure~\ref{fig:miniimagenet_prior_ours_chain} shows the conditional denoising process at regularly spaced intervals, and Figure~\ref{fig:miniimagenet_prior_ours_chain_unconditional} shows unconditional denoising. Each image of the chain is generated by passing the predicted $\latentdis^t$ through the VQ-VAE decoder.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/ours/chains2.png}
    \caption{Sampling denoising chain from $t=500$ up to $t=0$, shown at regular intervals, conditioned on the outer part of the picture. We show only the last $500$ steps of this process, as the first $500$ steps are not visually informative. The sampling procedure is described in Appendix~\ref{ap:inpainting}.}
    \label{fig:miniimagenet_prior_ours_chain}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/ours/chains_unconditional.png}
    \caption{Sampling denoising chain from $t=500$ up to $t=0$, shown at regular intervals, unconditional. We show only the last $500$ steps of this process, as the first $500$ steps are not visually informative. The sampling procedure is described in Algorithm~\ref{alg:sample}}
    \label{fig:miniimagenet_prior_ours_chain_unconditional}
\end{figure}



\section{Conclusion}
This work introduces a new mathematical framework for VQ-VAEs which includes a diffusion probabilistic model to learn the dependencies between the continuous latent  variables alongside the encoding and decoding part of the model. We showed conceptual improvements of our model over the VQ-VAE prior, as well as first numerical results on middle scale image generation. We believe that these first numerical experiments open up many research avenues: scaling to larger models, optimal scaling of the hyperparameters, including standard tricks from other diffusion methods, studying the influence of regulazation loss for end-to-end training, etc. We hope that this framework will serve as a sound and stable foundation to derive future generative models.
% \paragraph{CelebHQ dataset}

% larger pretrained vqvae encoder/decoder (celeb? Imagenet?) for nice images (no comparison) \todo{anyone}

% As the conditional law $p_{\theta}(Z_q^{0:T},Z_e^{0:T}| X)$ is not available explicitly, this work focuses on  variational approaches to provide an approaximation. Consider first an encoding function $f_\eta$ and write $Z_\rme= f_\eta(X)$. Then, consider the following variational familly:
% $$
% q_{\varphi}(Z_q^{0:T},Z_e^{0:T}| X) = q^u_{\varphi,0}(Z_e^0 | Z_\rme)q_{\varphi,0}^z(Z_q^0|Z_e^0) \prod_{t=1}^T\left\{ q^t_{\varphi,t}(Z_e^t|Z_e^{t-1})q^z_{\varphi,t}(Z_q^t|Z_e^t)\right\}\,.
% $$
% Using the auxiliary variational inference framework (AVI) and following \cite{}, define the loss function:
% \begin{multline*}
% \mathcal{L}(\theta, \varphi) = \mathbb{E}_{q_\varphi} \left[ \log p_\theta(Z_e^{0},Z_q^{0}|Z_e^{0},Z_q^{0})\right]\\
%  - \sum_{t=1}^{T}\mathbb{E}_{q_\varphi} \left[\mathrm{KL}(q_\varphi(Z_q^{t-1},Z_e^{t-1}|Z_q^t,Z_e^{t},Z_q^0,Z_e^{0})\|p_\theta(Z_q^{t-1},Z_e^{t-1}|Z_q^t,Z_e^t)) \right]\,.
% \end{multline*}
% In our case, this loss reduces to
% $$
% \mathcal{L}(\theta, \varphi) = \mathbb{E}_{q_\varphi} \left[ \log p_\theta(Z_e^{0},Z_q^{0}|Z_e^{0},Z_q^{0}) - \sum_{t=1}^{T}\mathrm{KL}(q^u_{\varphi,t}(Z_e^{t-1}|Z_e^{t},Z_e^{0})\|p^u_{\theta,t}(Z_e^{t-1}|Z_e^t)) \right]\,.
% $$
% Bridge sampling quantizers then proceed as follows.
% \begin{enumerate}
% \item Define the noising transition density $q^u_{\varphi,t}(Z_e^{t}|Z_e^{t-1})$ as the transition density of a SDE.
% \item Compute the bridge sampler $q^u_{\varphi,t}(Z_e^{t-1}|Z_e^{t},Z_e^{0})$ or approximate it using Monte Carlo simulation and/or discretization schemes.
% \item Define $p^u_{\theta,t}(Z_e^{t-1}|Z_e^t)$ as $q_\varphi(Z_e^{t-1}|Z_e^{t},Z_e^{0})_{|Z_e^{0} = g_{\theta,t}(Z_e^t)}$ where $g_{\theta,t}$ is a deep neural network (see below) provides a denoised latent state from $Z_e^t$.
% \end{enumerate}
\clearpage
\newpage
\bibliography{vqvae}
\bibliographystyle{icml2022}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Details on the loss function}
\label{ap:loss}
\begin{proof}[Proof of Lemma~\ref{lem:loss}]
By definition,
$$
\mathcal{L}(\theta,\varphi) = \mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta}(\latentdis^{0:T},\latentcont^{0:T},x)}{q_{\varphi}(\latentdis^{0:T},\latentcont^{0:T}| x)}\right]\,,
$$
which yields
$$
 \mathcal{L}(\theta,\varphi) = \mathbb{E}_{q_{\varphi}}\left[\log p^x_{\theta}(x|\latentdis^{0})\right]   + \mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentdis}_{\theta}(\latentdis^{0:T}|\latentcont^{0:T})}{q^{\latentdis}_{\varphi}(\latentdis^{0:T}|\latentcont^{0:T})}\right] +\mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta}(\latentcont^{0:T})}{q^{\latentcont}_{\varphi}(\latentcont^{0:T}| x)}\right]\,.
$$
The last term may be decomposed as
$$
\mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta}(\latentcont^{0:T})}{q^{\latentcont}_{\varphi}(\latentcont^{0:T}| x)}\right] = \mathbb{E}_{q_{\varphi}}\left[\log p^{\latentcont}_{\theta,T}(\latentcont^{T})\right] + \sum_{t=1}^T \mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta,t-1|t}(\latentcont^{t-1}|\latentcont^{t})}{q^{\latentcont}_{\varphi,t|t-1}(\latentcont^{t}|\latentcont^{t-1})}\right]
$$
and
$$
\mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta}(\latentcont^{0:T})}{q^{\latentcont}_{\varphi}(\latentcont^{0:T}| x)}\right] = \mathbb{E}_{q_{\varphi}}\left[\log p^{\latentcont}_{\theta,T}(\latentcont^{T})\right] +\mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta,0|1}(\latentcont^{0}|\latentcont^{1})}{q^{\latentcont}_{\varphi,1|0}(\latentcont^{1}|\latentcont^{0})}\right] + \sum_{t=2}^T \mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta,t-1|t}(\latentcont^{t-1}|\latentcont^{t})}{q^{\latentcont}_{\varphi,t|t-1}(\latentcont^{t}|\latentcont^{t-1})}\right]\,.
$$
By \eqref{eq:markov:bridge},
$$
\mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta}(\latentcont^{0:T})}{q^{\latentcont}_{\varphi}(\latentcont^{0:T}| x)}\right] = \mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta,T}(\latentcont^{T})}{q^{\latentcont}_{\varphi,T|0}(\latentcont^{T}|\latentcont^{0})}\right] + \sum_{t=2}^T \mathbb{E}_{q_{\varphi}}\left[\log \frac{p^{\latentcont}_{\theta,t-1|t}(\latentcont^{t-1}|\latentcont^{t})}{q^{\latentcont}_{\varphi,t-1|0,t}(\latentcont^{t-1}|\latentcont^{0},\latentcont^{t})}\right] + \mathbb{E}_{q_{\varphi}}\left[\log p^{\latentcont}_{\theta,0|1}(\latentcont^{0}|\latentcont^{1})\right]\,,
$$
which concludes the proof.
\end{proof}

\section{Inpainting diffusion sampling}
\label{ap:inpainting}

We consider the case in which we know a sub-part of the picture $\overline{X}$, and want to predict the complementary pixels $\underline{X}$. Knowing the corresponding $n$ latent vectors $\overline{\latentcont}^0$ which result from $\underline{X}$ through the encoder, we sample $N - n$ $\underline{\latentcont}^T$ from the uninformative distribution $\underline{\latentcont}^T \sim \mathcal{N}(0, (2\vartheta)^{-1}\eta^2 \mathbf{I}_{d \times (N - n)})$. In order to produce the chain of samples $\latentcont^{t-1}$ from $\latentcont^t$ we then follow the following procedure.
\begin{itemize}
\item $\underline{\latentcont}^{t-1}$ is predicted from $\latentcont^t$ using the neural network predictor, similar to the unconditioned case. %Only the unknown part $\underline{\latentcont}^{t-1}$ is kept.
\item Sample $\overline{\latentcont}^{t-1}$ using the forward bridge noising process.
%\item $\latentcont^{t-1} = \overline{\latentcont}^{t-1} \cup  \underline{\latentcont}^{t-1}$
\end{itemize}


% Alternative: For our prior, we sample at time $T$ from the uninformative distribution  and start the denoising process. At each time step $t$ of this process, we sample as usual from $p_{\theta,t-1|t}^{\latentcont}(\latentcont^{t-1}|\latentcont^{t})$ and mask it with a sample from $\bckw^{\latentcont}_{\varphi,t|0,T}(\latentcont^t|\latentcont^0,\latentcont^T)$ or using the forward trajectory obtained with the transition density $\bckw^{\latentcont}_{\varphi,t|t-1}$. In our experiments, we did not notice significant differences using one or the other.%

\section{Additional regularisation considerations}
\label{ap:reg}

We consider here details about the parameterisation of $p_{\theta}^{\latentdis}(\latentdis^t|\latentcont^t)$ and $q_{\varphi}^{\latentdis}(\latentdis^t|\latentcont^t)$ in order to compute $\mathcal{L}^{reg}_t(\theta,\varphi)$.
Using the Gumbel-Softmax formulation provides an efficient and differentiable parameterisation. 
\begin{align*}
p_{\theta,t}^{\latentdis}(\latentdis^t = \cdot|\latentcont^t) &= \mathrm{Softmax}\{(-\|\latentcont - \embed_k\|^2_2 + G_k )/\tau_t\}_{1\leqslant k \leqslant K}\,,\\
q_{\varphi,t}(\latentdis^t = \cdot|\latentcont^t) &= \mathrm{Softmax}\{(-\|\latentcont - \embed_k\|^2_2 + \tilde G_k )/\tau\}_{1\leqslant k \leqslant K}\,,
\end{align*}
 where $\{(G_k,\tilde G_k)\}_{1\leqslant k \leqslant K}$ are i.i.d. with distribution $\mathrm{Gumbel}(0,1)$, $\tau>0$, and $\{\tau_t\}_{0\leqslant t \leqslant T}$ are positive  time-dependent scaling parameters. Then, up to the additive normalizing terms,
\begin{align*} 
\mathcal{L}^{reg}_t(\theta,\varphi) = \mathbb{E}_{q_{\varphi}}\left[\log \frac{p_{\theta,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}{q_{\varphi,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})}\right] &= \left(-\frac{1}{\tau_t} + \frac{1}{\tau}\right)\|\latentcont^t - \widehat{\latentdis^t}\|_2^2   - \frac{\tilde G_k}{\tau} + \frac{G_k}{\tau_t}\,,
%\\&+ \log \left(\sum_{k=1}^{K}\exp\left\{\frac{-\|\latentcont^t-\embed_k\|_2 + \tilde G_k}{\tau} \right\}\right) \\&+ \log \left(\sum_{k=1}^{K}\exp\left\{\frac{-\|\latentcont^t-\embed_k\|_2 + G_k)}{\tau_t}\right\}\right)
\end{align*}
where $\widehat{\latentdis^t}\sim q_{\varphi,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})$. Considering only the first term which depend on $\latentcont^t$ and produce non-zero gradients, we get:
$$
\mathcal{L}^{reg}_t(\theta,\varphi) = \gamma_t \|\latentcont^t - \widehat{\latentdis^t}\|_2^2
$$
where $\gamma_t = -1/\tau_t + 1/\tau$ drives the behavior of the regulariser. By choosing is $\gamma_t$ negative for large $t$, the regulariser pushes the codebooks away from $\latentcont^t$, which prevents too early specialization, or matching of codebooks with noise, as $\latentcont^{t \approx T}$ is close to the uninformative distribution. Finally, for small $t$, choosing $\gamma_t$ positive helps matching codebooks with $\latentcont$ when the corruption is small. In practice $\tau=1$ and a simple schedule from $10$ to $0.1$ for $\tau_t$ was considered in this work.

\section{Neural Networks}
\label{ap:networks}
For $\varepsilon_\theta(\latentcont^t ,t)$, we use a U-net like architecture similar to the one mentioned in \cite{ho2020denoising}. It consists of a deep convolutional neural network with 57M parameters, which is slightly below the PixelCNN architecture (95.8M parameters). The VQ-VAE encoder / decoders are also deep convolutional networks totalling 65M parameters.

\section{Toy Example Appendix}
\label{ap:additionaltoy}

\paragraph{Parameterisation}

We consider a neural network to model  $\varepsilon_\theta(\latentcont^t ,t)$. The network shown in Figure  \ref{ap:fig:toynetwork} consists of a time embedding similar to \cite{ho2020denoising}, as well as a few linear or 1D-convolutional layers, totalling around $5000$ parameters.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=2.0]{images/network_toy.png}
    \caption{Graphical representation of the neural network used for the toy dataset.}
    \label{ap:fig:toynetwork}
\end{figure}

For the parameterisation of the quantisation part, we choose $p_{\theta,t}^{\latentdis}(\latentdis^{t}=\embed_j|\latentcont^{t}) = \mathrm{Softmax}_{1\leq k \leq K}\{-\|\latentcont - \embed_k\|_2\}_j$, and the same parameterisation for $q_{\varphi,t}^{\latentdis}(\latentdis^{t}|\latentcont^{t})$. Therefore our loss simplifies to:
$$
\mathcal{L}(\theta,\varphi) = \mathbb{E}_{q_{\varphi}}\left[\log p^x_{\theta}(x|\latentdis^{0})\right] +  \mathcal{L}_t(\theta,\varphi)\,,
$$

where $t$ is sampled uniformly in $\{0,\ldots,T\}$.

\paragraph{Discrete samples during diffusion process}

\begin{table}[h!]
    
    \centering
    \begin{tabular}{l|c}
        t & NN sequence \\
        \hline
        50& (0, 7, 3, 6, 2)\\
        40& (6, 5, 5, 5, 3)\\
        30& (5, 5, 5, 4, 2)\\
        20& (6, 6, 5, 4, 3)\\
        10& (5, 6, 5, 4, 3)\\
        0& (5, 6, 5, 4, 3)
    \end{tabular}
    \caption{\label{ap:tab:discretetoy} Discrete samples during diffusion process. The discrete sequence is obtained by computing the nearest neighbour centroid $\mu_j$ for each $X^t_s$. At $t=0$, $X^0$ is sampled from a centered Gaussian distribution with small covariance matrix $(2\vartheta)^{-1}\eta^2\mathbf{I}_{2\times 5}$, resulting in a uniform discrete sequence, as all centroids have a similar unit norm.}

\end{table}


Discrete sequences corresponding to the denoising diffusion process shown in Figure \ref{fig:noisedenoise} are shown in Table~\ref{ap:tab:discretetoy}.

\paragraph{End-to-end training}
\label{ap:end2end}
In order to train the codebooks alongside the diffusion process, we need to backpropagate the gradient of the likelihood of the data $\latentcont$ given a $\latentcont^0$ reconstructed by the diffusion process (corresponding to $\mathcal{L}^{rec}(\theta,\varphi)$). We use the Gumbel-Softmax parameterisation in order to obtain a differentiable process and update the codebooks $\embed_j$.

In this toy example, the use of the third part of the loss $\sum_{t=0}^T \mathcal{L}^{reg}_t(\theta,\varphi)$ is not mandatory as we obtain good results with $\mathcal{L}^{reg}_t(\theta,\varphi) = 0$, which means parametrising $p_{\theta,t}^{\latentdis}(\latentdis^t|\latentcont^t) = q_{\varphi,t}^{\latentdis}(\latentdis^t|\latentcont^t)$. However we noticed that $\mathcal{L}^{reg}_t(\theta,\varphi)$ is useful to improve the learning of the codebooks. If we choose $\gamma_t$ to be decreasing with time $t$, we have the following. When $t$ is low, the denoising process is almost over, $\mathcal{L}^{reg}_t(\theta,\varphi)$ pushes $\latentcont$ and the selected $\latentdis$ to close together: $\|\latentcont\| \sim 1$, then $\|\latentcont^t\|$ will be likely near a specific $\embed_j$ and far from the others; therefore only a single codebook is selected and receives gradient. When $t$ is high, $\|\latentcont^t\| \sim 0$ and the Gumbel-Softmax makes it so that all codebooks are equidistant from $\|\latentcont^t\|$ and receive non-zero gradient. This naturally solves training problem associated with dead codebooks in VQ-VAEs. Joint training of the denoising and codebooks yield excellent codebook positionning as shown in Figure \ref{ap:codebooks}.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{images/codebookstraining.png}
    \caption{Left, initial random codebooks positions. Right, after training, position of codebook vectors. Note that the codebook indexes do not match the indexes of the Gaussians, the model learnt to make the associations between neighboring centroids in a different order.}
    \label{ap:codebooks}
\end{figure}

\paragraph{Toy Diffusion inpainting}

\label{ap:toyinpainting}
We consider a case in which we want to reconstruct an $x$ while we only know one (or a few) dimensions, and sample the others. Consider that $x$ is generated using a sequence $q= (q_1,q_2,q_",q_4,q_5)$ where the last one if fixed $q_1 = 0, q_5 = 4$. Then, knowing $q_1, q_5$, we sample $q_2,q_3,q_4$, as shown in Figure \ref{fig:fixeddim}.
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.5]{images/samplesFixed.png}
    \caption{Three independent sampling of $X$ using a trained diffusion bridge, with fixed $q_1 = 0, q_5 = 4$. The three corresponding sequences are $(0,7,6,5,4)$, $(0,1,2,3,4)$, $(0,7,6,5,4)$ all valid sequences.}
    \label{fig:fixeddim}
\end{figure}

 


% \section{On NLL evaluation}
% \paragraph{PixelCNN}%
% In PixelCNN, the authors clearly state that a single scalar value in produced for each pixel (and each channel): \textit{Each pixel $x_i$ is in turn jointly determined by three values, one for each of the color channels Red, Green and Blue (RGB).}.
% But one paragraph later, they seem to claim the opposite: \textit{we model $p(x)$ as a discrete distribution, with every conditional distribution being a multinomial that is modeled with a softmax layer. Each channel variable $x_i$ simply takes one of 256 distinct values}.
% Then, in Section 5.1, there is a discussion on comparing continuous and discrete distributions, by adding real valued noise, which I couldn't make sense of.
% In the face of all previous evidences, and considering they measure the NLL in \textit{bits/dim}, I have come to the conclusion that PixelCNN outputs a discrete distribution for each pixel, then compares it to the continuous distribution of the dataset.
% Since there is no latent space of interest, these NLL must be computed directly on image pixels.

% \paragraph{VQVAE}%
% The output type of the VQVAE model, as well as its evaluation methods are still unclear to me.
% On the one hand, the authors mention \textit{likelihood lower bounds} in \textit{bits/dim}, providing values matching those reported in PixelCNN.
% In a section called \textit{Comparison with continuous variables}, which I interpret as being a reference to the discrete nature of the latent space, they compare themselves to classic VAE and VIMCO.
% We can assume this metric refers to image pixels (same as PixelCNN), and not to the latent space.

% On the other hand, in the official implementation of the vqvae and the example notebook provided along, the model outputs a scalar value for each pixel.
% The only cost function used during training were the MSE and the commitment loss, no NLL was mentioned or computed.
% There is no doubt on the nature of that particular model since, when plotting images, they use an interpolation function.

% \paragraph{VQVAE-2}%
% In VQVAE-2, the NLL is \textit{not used for comparing} with state of the art, but only as an overfit sanity check.
% Indeed, the NLL seems to be computed on the latent spaces, as the authors mention \textit{We note that these NLL values are only comparable between prior models that use the same pretrained VQ-VAE encoder and decoder}.
% Instead of NLL, they use two kind of evaluation metrics:
% \begin{itemize}
% 	\item Corrected FID
% 	\item Classification Accuracy Score: for class-conditional generative models only.
% \end{itemize}

% \paragraph{Conclusion}%
% \textbf{The VQVAE} mentions having computed a likelihood lower bound, but no details are provided. The official implementation is not coherent with this metric, as only the MSE can be computed.
% \textbf{The VQVAE-2} computes the NLL, but only on its own latent space, thus can't be compared with any other model. Instead, other metrics are criticized, then used. In the end, the compare their own model using their own defined metrics, on a task relevant only to class-conditional generative models.


\section{Additional visuals}\label{ap:additional_visuals}

\subsection{Cifar}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\linewidth]{images/cifar_vqvae.png}
    \caption{Reconstruction of the VQVAE model used in the following benchmarks.}
    \label{fig:cifar_vqvae}
\end{figure}

\begin{figure}[!htb]
    \centering
    \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=1.\linewidth]{images/samples_pixel_cifar.png}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=1.\linewidth]{images/ours/samples_cifar.png}
    \end{minipage}
        \caption{Samples from the PixelCNN prior (left) and from our diffusion prior (right) on CIFAR10.}
        
    \label{fig:cifar_priors}
\end{figure}


\subsection{MiniImageNet}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\linewidth]{images/vqvae_miniImagenet_reconstruction.png}
    \caption{Reconstruction of the trained VQ-VAE on the \textit{mini}ImageNet dataset. Original images are encoded, discretised, and decoded.}
    \label{fig:miniimagenet_vqvae}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.\linewidth]{images/ours/samples.png}
    \caption{Samples from our model for the miniimagenet dataset}
    \label{fig:miniimagenet_prior_ours2}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1.\linewidth]{images/ours/cond_sampling2.png}
    \includegraphics[width=1.\linewidth]{images/cond_sampling_pixelcnn_train.png}
    \includegraphics[width=1.\linewidth]{images/cond_sampling_pixelcnn_train2.png}
    \caption{Conditional sampling: Top: reconstructions from the vqvae of originals images, Middle: conditional sampling with the left side of the image as condition, for our model. Bottom 1 and 2: conditional sampling in the same context with the PixelCNN prior.}
    \label{fig:miniimagenet_prior_ours_conditional2}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{images/ours/media_images_conditional_sampling_chain_14_4.png}
    \includegraphics[width=.8\linewidth]{images/ours/media_images_conditional_sampling_chain_14_0.png}
    \includegraphics[width=.8\linewidth]{images/ours/media_images_conditional_sampling_chain_13_4.png}
    \includegraphics[width=.8\linewidth]{images/ours/media_images_conditional_sampling_chain_14_1.png}
    \includegraphics[width=.8\linewidth]{images/ours/media_images_conditional_sampling_chain_12_1.png}
    \includegraphics[width=.8\linewidth]{images/ours/media_images_conditional_sampling_chain_10_4.png}
    \caption{Sampling denoising chain from up to $t=0$, shown at regular intervals, conditioned on the left part of the picture. The sampling procedure is described in Appendix~\ref{ap:inpainting}.}
    \label{fig:miniimagenet_prior_ours_chain2}
\end{figure}

\iffalse
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/ours/cond_samples2.png}
    \includegraphics[width=.8\linewidth]{images/cond_sampling_pixelcnn_train.png}
    \includegraphics[width=.8\linewidth]{images/cond_sampling_pixelcnn_train2.png}
    \caption{Conditional sampling: Top: reconstructions from the VQ-VAE of originals images, Middle: conditional sampling with the left side of the image as condition, for our model. Bottom 1 and 2: conditional sampling in the same context with the PixelCNN prior.}
    \label{fig:miniimagenet_prior_ours_conditional3}
\end{figure}
\fi

\begin{figure}
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=1.\linewidth]{images/cond_sampling_original.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}

    \includegraphics[width=1.\linewidth]{images/cond_sampling_pixelcnn.jpg}
        \end{minipage}
    \caption{Conditional sampling with the PixelCNN prior. \textbf{Left}: original images, \textbf{Right}: conditional sampling with the left side of the image as condition. Each row represents a class of the validation set of the \textit{mini}ImageNet dataset.}
    \label{fig:miniimagenet_prior_pixelcnn_conditional}

\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
